"""
å¤šæ¨¡æ€æ£€ç´¢ç®¡ç†å™¨ (MultivectorMgr)

è´Ÿè´£ä½¿ç”¨doclingè§£ææ–‡æ¡£ï¼Œå®ç°åˆ†å±‚åˆ†å—ç­–ç•¥ï¼Œç”Ÿæˆçˆ¶å—å’Œå­å—ï¼Œ
å¹¶è°ƒç”¨æ¨¡å‹è¿›è¡Œå›¾ç‰‡æè¿°ç”Ÿæˆå’Œæ–‡æœ¬å‘é‡åŒ–ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä½¿ç”¨doclingè§£æPDF/PPT/DOCXç­‰æ–‡æ¡£
2. åŸºäºdoclingçš„body/groupsç»“æ„å®ç°åˆ†å±‚åˆ†å—
3. è°ƒç”¨visionæ¨¡å‹ç”Ÿæˆå›¾ç‰‡/è¡¨æ ¼æè¿°
4. è°ƒç”¨embeddingæ¨¡å‹è¿›è¡Œå‘é‡åŒ–
5. å­˜å‚¨åˆ°SQLite(å…ƒæ•°æ®)å’ŒLanceDB(å‘é‡)
"""

from core.config import singleton, generate_vector_id, BUILTMODELS
import os
import json
import hashlib
import logging
from pathlib import Path
from datetime import datetime
from typing import (
    # Dict, 
    # Any, 
    List, 
    Optional, 
    Tuple,
)
from sqlmodel import Session, select
from sqlalchemy import Engine
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    PictureDescriptionApiOptions,
    PdfPipelineOptions,
)
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling_core.types.doc import (
    DoclingDocument,
    ImageRefMode,
    # PictureItem,
    # TableItem,
    # TextItem,
)
from docling.datamodel.document import ConversionResult
from docling.chunking import HybridChunker
from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer
from transformers import AutoTokenizer
from core.db_mgr import Document, ParentChunk, ChildChunk, ModelCapability
from core.lancedb_mgr import LanceDBMgr
from core.models_mgr import ModelsMgr
from core.model_config_mgr import ModelConfigMgr
from multiprocessing import Lock as ProcessLock
import asyncio

logger = logging.getLogger()
# åˆ›å»ºè¿›ç¨‹çº§é” (å¿…é¡»åœ¨æ¨¡å—çº§åˆ«åˆ›å»ºï¼Œä»¥ä¾¿å­è¿›ç¨‹ç»§æ‰¿)
_metal_gpu_lock = ProcessLock()

def acquire_metal_lock(operation: str):
    """è·å– Metal GPU é” (åŒæ­¥ç‰ˆæœ¬)"""
    logger.info(f"[METAL_LOCK] Acquiring lock for: {operation}")
    _metal_gpu_lock.acquire()
    logger.info(f"[METAL_LOCK] Lock acquired for: {operation}")

def release_metal_lock(operation: str):
    """é‡Šæ”¾ Metal GPU é” (åŒæ­¥ç‰ˆæœ¬)"""
    _metal_gpu_lock.release()
    logger.info(f"[METAL_LOCK] Lock released for: {operation}")

# å¼‚æ­¥ç‰ˆæœ¬ (ç”¨äº async/await ä¸Šä¸‹æ–‡)
async def acquire_metal_lock_async(operation: str):
    """è·å– Metal GPU é” (å¼‚æ­¥ç‰ˆæœ¬)"""
    logger.info(f"[METAL_LOCK] Acquiring lock for: {operation}")
    # åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­ç­‰å¾…é”
    loop = asyncio.get_event_loop()
    await loop.run_in_executor(None, _metal_gpu_lock.acquire)
    logger.info(f"[METAL_LOCK] Lock acquired for: {operation}")

async def release_metal_lock_async(operation: str):
    """é‡Šæ”¾ Metal GPU é” (å¼‚æ­¥ç‰ˆæœ¬)"""
    _metal_gpu_lock.release()
    logger.info(f"[METAL_LOCK] Lock released for: {operation}")


os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ============================================================================
# æ¨¡å—çº§å‡½æ•°ï¼šç”¨äºå­è¿›ç¨‹æ‰§è¡Œï¼ˆé¿å…åµŒå¥—å‡½æ•°åºåˆ—åŒ–é—®é¢˜ï¼‰
# ============================================================================

def _docling_worker_func(file_path: str, ocr_options: dict, use_proxy: bool, proxy_value: str, result_queue):
    """
    åœ¨å­è¿›ç¨‹ä¸­è¿è¡ŒDoclingè§£æï¼ˆæ¨¡å—çº§å‡½æ•°ï¼Œå¯è¢«multiprocessingåºåˆ—åŒ–ï¼‰
    
    è¿™ä¸ªå‡½æ•°ä¼šåœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œæ‹¥æœ‰å®Œå…¨ç‹¬ç«‹çš„Metalä¸Šä¸‹æ–‡ï¼Œ
    ä¸ä¼šä¸ä¸»è¿›ç¨‹ä¸­çš„MLX-VLMäº§ç”ŸMetal GPUå‘½ä»¤ç¼–ç å™¨å†²çªã€‚
    """
    try:
        # å­è¿›ç¨‹ä¸­é‡æ–°å¯¼å…¥å’Œåˆå§‹åŒ–
        from docling.document_converter import DocumentConverter, PdfFormatOption
        from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline
        from docling.datamodel.base_models import InputFormat
        from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions
        import os
        import pickle
        
        # é…ç½®OCR
        pipeline_options = PdfPipelineOptions()
        if ocr_options.get("do_ocr", False):
            pipeline_options.do_ocr = True
            easyocr_options = EasyOcrOptions(
                lang=ocr_options.get("ocr_lang", ["ch_sim", "en"])
            )
            pipeline_options.ocr_options = easyocr_options
        
        # åˆ›å»ºè½¬æ¢å™¨
        converter = DocumentConverter(
            allowed_formats=[InputFormat.PDF],
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_cls=StandardPdfPipeline,
                    pipeline_options=pipeline_options,
                )
            },
        )
        
        # è®¾ç½®ä»£ç†
        if use_proxy and proxy_value:
            os.environ['ALL_PROXY'] = proxy_value
        
        # æ‰§è¡Œè§£æ
        result = converter.convert(source=file_path)
        
        # ğŸ”§ åªåºåˆ—åŒ– document çš„å­—å…¸è¡¨ç¤ºï¼Œé¿å… pickle æ•´ä¸ª result å¯¹è±¡
        # result å¯¹è±¡åŒ…å«æ— æ³• pickle çš„ PDF parser å¼•ç”¨
        doc_dict = result.document.export_to_dict()
        result_queue.put(("success", pickle.dumps(doc_dict)))
        
    except Exception as e:
        result_queue.put(("error", str(e)))
    finally:
        os.environ.pop('ALL_PROXY', None)

# ä¸åŒä¸šåŠ¡åœºæ™¯æ‰€éœ€æ¨¡å‹èƒ½åŠ›çš„ç»„åˆ
SCENE_MULTIVECTOR: List[ModelCapability] = [ModelCapability.TEXT, ModelCapability.VISION]

# Doclingæ”¯æŒçš„æ–‡ä»¶æ ¼å¼, https://docling-project.github.io/docling/examples/run_with_formats/
SUPPORTED_FORMATS = ['pdf', 'docx', 'pptx', 'txt', 'md', 'markdown']

@singleton
class MultiVectorMgr:
    """å¤šæ¨¡æ€åˆ†å—ç®¡ç†å™¨"""

    def __init__(self, engine: Engine, lancedb_mgr: LanceDBMgr, models_mgr: ModelsMgr):
        """
        åˆå§‹åŒ–åˆ†å—ç®¡ç†å™¨
        
        Args:
            engine: SQLAlchemyæ•°æ®åº“å¼•æ“
            lancedb_mgr: LanceDBå‘é‡æ•°æ®åº“ç®¡ç†å™¨
            models_mgr: æ¨¡å‹ç®¡ç†å™¨ï¼ˆç”¨äºè°ƒç”¨visionå’Œembeddingæ¨¡å‹ï¼‰
        """
        self.engine = engine
        self.lancedb_mgr = lancedb_mgr
        self.models_mgr = models_mgr
        self.model_config_mgr = ModelConfigMgr(engine)
        # åœ¨ç”¨æˆ·æŒ‡å®švisionæ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦åˆå§‹åŒ–æ‰èƒ½ä½¿ç”¨
        self.converter = None
        self.use_proxy = False
        # è·å–æ•°æ®åº“ç›®å½•ä½œä¸ºåŸºç¡€è·¯å¾„
        self._init_base_paths()
        
        # åˆå§‹åŒ–chunker
        self._init_chunker()
        
        logger.info("MultivectorMgr initialized successfully")
    
    def check_multivector_model_availability(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹ã€‚
        å¦‚æœæ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œè¿”å›Falseå¹¶è®°å½•è­¦å‘Šã€‚
        """
        for capa in SCENE_MULTIVECTOR:
            if self.model_config_mgr.get_spec_model_config(capa) is None:
                logger.warning(f"Model for multivector is not available: {capa}")
                return False
        return True
    
    def _init_base_paths(self):
        """åˆå§‹åŒ–åŸºç¡€è·¯å¾„ï¼Œä½¿ç”¨æ•°æ®åº“ç›®å½•çš„çˆ¶ç›®å½•"""
        try:
            # å°è¯•ä»LanceDBè·å–è·¯å¾„
            if hasattr(self.lancedb_mgr, 'db') and hasattr(self.lancedb_mgr.db, 'uri'):
                lancedb_path = Path(self.lancedb_mgr.db.uri)
                self.data_base_dir = lancedb_path.parent
            
            # åˆ›å»ºdoclingç¼“å­˜ç›®å½•
            self.docling_cache_dir = self.data_base_dir / "docling_cache"
            self.docling_cache_dir.mkdir(exist_ok=True)
            
            logger.info(f"Data base directory: {self.data_base_dir}")
            logger.info(f"Docling cache directory: {self.docling_cache_dir}")
            
        except Exception as e:
            logger.error(f"Failed to initialize base paths: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆ
            self.data_base_dir = Path.cwd()
            self.docling_cache_dir = self.data_base_dir / "docling_cache"
            self.docling_cache_dir.mkdir(exist_ok=True)
    
    def _init_docling_converter(self):
        """åˆå§‹åŒ–doclingæ–‡æ¡£è½¬æ¢å™¨"""

        try:
            # è·å–å½“å‰è§†è§‰æ¨¡å‹é…ç½®
            model_interface = self.model_config_mgr.get_vision_model_config()
            vision_model_id = model_interface.model_identifier
            vision_base_url = model_interface.base_url
            vision_api_key = model_interface.api_key
            self.use_proxy = model_interface.use_proxy

            # é…ç½®PDFå¤„ç†é€‰é¡¹
            pipeline_options = PdfPipelineOptions()
            pipeline_options.generate_picture_images = True
            # pipeline_options.generate_page_images = True
            pipeline_options.images_scale = 2.0  # å›¾ç‰‡åˆ†è¾¨ç‡scale
            pipeline_options.do_picture_description = True
            pipeline_options.enable_remote_services = True  # å¯ç”¨è¿œç¨‹æœåŠ¡ç”¨äºå›¾ç‰‡æè¿°
            params=dict(
                model=vision_model_id,
                seed=42,
                temperature=0.2,
                max_completion_tokens=250,
                api_key=vision_api_key,
            )
            pipeline_options.picture_description_options = PictureDescriptionApiOptions(
                url=f"{vision_base_url}/chat/completions",
                params=params,
                prompt="""
You are an assistant tasked with summarizing images for retrieval. 
These summaries will be embedded and used to retrieve the raw image. 
Give a concise summary of the image that is well optimized for retrieval.
""".strip(),
                timeout=180,
            )
            pipeline_options.do_ocr = False  # å…³é—­OCRï¼Œä¾èµ–doclingå†…ç½®çš„ç®€å•OCR
            
            # åˆ›å»ºæ–‡æ¡£è½¬æ¢å™¨
            self.converter = DocumentConverter(format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=pipeline_options,
                )
            })
            
            logger.info("Docling converter initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize docling converter: {e}")
            raise
    
    def _init_chunker(self):
        """åˆå§‹åŒ–DoclingåŸç”Ÿchunkerï¼ŒåŸºäºæœ€ä½³å®è·µé…ç½®"""
        try:
            # chunkerçš„tokenizerä¸embeddingæ¨¡å‹å¯ä»¥ä¸æ˜¯åŒä¸€ä¸ª
            # HybridChunkerçš„tokenizerä¸»è¦ç”¨äºchunkå¤§å°æ§åˆ¶ï¼Œä¸éœ€è¦ä¸embeddingæ¨¡å‹å®Œå…¨ä¸€è‡´
            # ä½¿ç”¨é€šç”¨tokenizerè¿›è¡Œè¿‘ä¼¼ä¼°ç®—æ›´ç¨³å®šå¯é ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å†…ç½®çš„ä¸­è‹±æ–‡å‹å¥½tokenizerä½œä¸ºchunkå¤§å°ä¼°ç®—å™¨
            model_path = self.model_config_mgr.get_embeddings_model_path()
            if model_path == "":
                # ä½¿ç”¨lancedb_mgrçš„base_dirä½œä¸ºç¼“å­˜ç›®å½•ï¼Œå®ƒä¸SQLiteæ•°æ®åº“åœ¨åŒä¸€çˆ¶ç›®å½•
                cache_directory = self.lancedb_mgr.base_dir
                model_path = self.models_mgr.download_huggingface_model(BUILTMODELS['EMBEDDING_MODEL']['MLXCOMMUNITY'], cache_directory)
                self.model_config_mgr.set_embeddings_model_path(model_path)  
            tokenizer = HuggingFaceTokenizer(
                tokenizer=AutoTokenizer.from_pretrained(model_path),
                max_tokens=512,  # ä¿å®ˆçš„chunkå¤§å°ï¼Œç¡®ä¿embedding APIè°ƒç”¨ç¨³å®š
            )
            
            # åˆ›å»ºHybridChunkerå®ä¾‹
            self.chunker = HybridChunker(
                tokenizer=tokenizer,
                merge_peers=True,  # åˆå¹¶ç›¸é‚»çš„åŒç±»chunk
            )
            
            logger.info(f"Chunker initialized with max_tokens={tokenizer.get_max_tokens()}")
            
        except Exception as e:
            logger.error(f"Failed to initialize chunker: {e}")
            raise
    
    def process_document(self, file_path: str, task_id: str = None) -> bool:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£çš„å®Œæ•´æµç¨‹
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
            task_id: ä»»åŠ¡IDï¼Œç”¨äºäº‹ä»¶è¿½è¸ª
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        
        # åˆ¤æ–­æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ
        file_ext = Path(file_path).suffix.split('.')[-1].lower()
        if file_ext not in SUPPORTED_FORMATS:
            logger.warning(f"[MULTIVECTOR] Unsupported file type: {file_ext}")
            return False

        try:
            logger.info(f"[MULTIVECTOR] Starting document processing: {file_path}")
            
            # 1. éªŒè¯æ–‡ä»¶
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # 2. è®¡ç®—æ–‡ä»¶hash
            file_hash = self._calculate_file_hash(file_path)
            
            # 3. æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡ä¸”æ–‡ä»¶æœªå˜æ›´
            existing_doc = self._get_existing_document(file_path, file_hash)
            if existing_doc:
                logger.info(f"[MULTIVECTOR] Document already processed and unchanged: {file_path}")
                return True
            
            # 4. ä½¿ç”¨doclingè§£ææ–‡æ¡£
            docling_result = self._parse_with_docling(file_path)
            
            # 5. ä¿å­˜doclingè§£æç»“æœ
            docling_json_path = self._save_docling_result(file_path, docling_result)
            
            # 6. åˆ›å»º/æ›´æ–°Documentè®°å½•
            document = self._create_or_update_document(file_path, file_hash, docling_json_path)
            
            # 7. ç”Ÿæˆçˆ¶å—å’Œå­å—
            parent_chunks, child_chunks = self._generate_chunks(document.id, docling_result.document)
            
            # 8. å­˜å‚¨åˆ°æ•°æ®åº“
            self._store_chunks(parent_chunks, child_chunks)
            
            # 8.5. ä¸ºå›¾ç‰‡chunksåˆ›å»ºå›¾æ–‡å…³ç³»å­å—ï¼ˆå…³é”®è®¾è®¡ï¼‰
            all_parent_chunks, all_child_chunks = self._create_image_context_chunks(parent_chunks, child_chunks, document.id)
            
            # å¦‚æœåˆ›å»ºäº†é¢å¤–çš„ä¸Šä¸‹æ–‡å—ï¼Œæ›´æ–°chunkåˆ—è¡¨
            if len(all_parent_chunks) > len(parent_chunks):
                additional_parent_chunks = all_parent_chunks[len(parent_chunks):]
                additional_child_chunks = all_child_chunks[len(child_chunks):]
                
                # å­˜å‚¨é¢å¤–çš„ä¸Šä¸‹æ–‡å—
                self._store_chunks(additional_parent_chunks, additional_child_chunks)
                logger.info(f"Stored {len(additional_parent_chunks)} additional image context chunks")
                
                # æ›´æ–°ç”¨äºå‘é‡åŒ–çš„chunkåˆ—è¡¨
                parent_chunks = all_parent_chunks
                child_chunks = all_child_chunks
            
            # 9. å‘é‡åŒ–å’Œå­˜å‚¨
            self._vectorize_and_store(parent_chunks, child_chunks)
            
            # 10. æ›´æ–°æ–‡æ¡£çŠ¶æ€
            document.status = "done"
            document.processed_at = datetime.now()
            with Session(self.engine) as session:
                session.add(document)
                session.commit()
            
            logger.info(f"[MULTIVECTOR] Document processing completed: {file_path}")
            
            return True
            
        except Exception as e:
            logger.error(f"[MULTIVECTOR] Document processing failed for {file_path}: {e}", exc_info=True)
            
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºé”™è¯¯
            try:
                document = self._get_or_create_document_record(file_path, "", "")
                document.status = "error"
                with Session(self.engine) as session:
                    session.add(document)
                    session.commit()
            except Exception as e:
                logger.error(f"Failed to update document status: {e}")
                pass  # å¿½ç•¥çŠ¶æ€æ›´æ–°é”™è¯¯
            
            return False
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶hashå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                # å¯¹äºå¤§æ–‡ä»¶ï¼Œåªè¯»å–éƒ¨åˆ†å†…å®¹è®¡ç®—hash
                chunk_size = 8192
                while chunk := f.read(chunk_size):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logger.error(f"Failed to calculate hash for {file_path}: {e}")
            return ""
    
    def _get_existing_document(self, file_path: str, file_hash: str) -> Optional[Document]:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒhashçš„æ–‡æ¡£è®°å½•"""
        try:
            stmt = select(Document).where(
                Document.file_path == file_path,
                Document.file_hash == file_hash,
                Document.status == "done"
            )
            with Session(self.engine) as session:
                return session.exec(stmt).first()
        except Exception as e:
            logger.error(f"Failed to check existing document: {e}")
            return None
    
    def _parse_with_docling(self, file_path: str) -> ConversionResult:
        """ä½¿ç”¨doclingè§£ææ–‡æ¡£ï¼ˆåœ¨å­è¿›ç¨‹ä¸­è¿è¡Œä»¥é¿å…Metal GPUå†²çªï¼‰"""
        
        # ğŸš€ ä½¿ç”¨å­è¿›ç¨‹è¿è¡ŒDoclingï¼Œå®Œå…¨éš”ç¦»Metalä¸Šä¸‹æ–‡
        # ğŸ”’ ä½¿ç”¨å…¨å±€é”ç¡®ä¿ä¸ MLX-VLM äº’æ–¥
        from multiprocessing import Process, Queue
        import pickle
        
        # ğŸ”’ è·å– Metal GPU é”
        acquire_metal_lock("Docling PDF parsing")
        
        try:
            logger.info(f"[MULTIVECTOR] Parsing document with docling in subprocess: {file_path}")
            
            # å‡†å¤‡OCRé…ç½®
            ocr_options = {
                # "do_ocr": self.do_ocr,
                # "ocr_lang": self.ocr_lang
            }
            
            # è·å–ä»£ç†é…ç½®
            proxy_value = ""
            if self.use_proxy:
                proxy = self.model_config_mgr.get_proxy_value()
                if proxy and proxy.value:
                    proxy_value = proxy.value
            
            # åˆ›å»ºç»“æœé˜Ÿåˆ—
            result_queue = Queue()
            
            # åˆ›å»ºå¹¶å¯åŠ¨å­è¿›ç¨‹ï¼ˆä½¿ç”¨æ¨¡å—çº§å‡½æ•°ï¼‰
            process = Process(
                target=_docling_worker_func,
                args=(file_path, ocr_options, self.use_proxy, proxy_value, result_queue)
            )
            process.daemon = False  # ç¡®ä¿å­è¿›ç¨‹ç‹¬ç«‹è¿è¡Œ
            process.start()
            logger.info(f"[MULTIVECTOR] Docling worker process started (PID: {process.pid})")
            
            # ç­‰å¾…ç»“æœï¼ˆè®¾ç½®60ç§’è¶…æ—¶ï¼‰
            try:
                process.join(timeout=60)
            except Exception as e:
                logger.error(f"Error waiting for Docling subprocess: {e}")
                if process.is_alive():
                    process.terminate()
                    process.join(timeout=5)
                    if process.is_alive():
                        process.kill()  # å¼ºåˆ¶æ€æ­»
                raise RuntimeError(f"Docling subprocess join failed: {e}")
            
            if process.is_alive():
                # è¶…æ—¶ï¼Œå¼ºåˆ¶ç»ˆæ­¢
                logger.warning("Docling parsing timed out, terminating subprocess")
                process.terminate()
                process.join(timeout=5)
                if process.is_alive():
                    process.kill()
                raise TimeoutError("Docling parsing timed out after 60 seconds")
            
            # æ£€æŸ¥è¿›ç¨‹é€€å‡ºç 
            logger.info(f"[MULTIVECTOR] Docling worker exited with code: {process.exitcode}")
            if process.exitcode != 0:
                # å­è¿›ç¨‹å¼‚å¸¸é€€å‡ºï¼Œä½†ä¸è¦è®©å®ƒå½±å“ä¸»è¿›ç¨‹
                error_msg = f"Docling worker process failed with exit code {process.exitcode}"
                if process.exitcode == 134:
                    error_msg += " (SIGABRT - possible Metal GPU conflict or assertion failure)"
                elif process.exitcode < 0:
                    error_msg += f" (killed by signal {-process.exitcode})"
                logger.error(error_msg)
                raise RuntimeError(error_msg)
            
            # è·å–ç»“æœ
            if result_queue.empty():
                logger.error("Docling worker did not return any result in queue")
                raise RuntimeError("Docling worker did not return any result")
            
            try:
                status, data = result_queue.get(timeout=5)
                logger.info(f"[MULTIVECTOR] Got result from worker, status: {status}")
            except Exception as e:
                logger.error(f"Failed to get result from queue: {e}")
                raise RuntimeError(f"Failed to get result from Docling worker: {e}")
            
            if status == "error":
                logger.error(f"Docling worker returned error: {data}")
                raise RuntimeError(f"Docling parsing failed: {data}")
            
            # ååºåˆ—åŒ–ç»“æœï¼ˆç°åœ¨æ˜¯ document dictï¼Œè€Œä¸æ˜¯å®Œæ•´çš„ ConversionResultï¼‰
            logger.info("[MULTIVECTOR] Deserializing Docling result from subprocess")
            import pickle
            from docling_core.types.doc import DoclingDocument
            
            try:
                doc_dict = pickle.loads(data)
                logger.info(f"[MULTIVECTOR] Successfully unpickled document dict, keys: {list(doc_dict.keys()) if isinstance(doc_dict, dict) else 'not a dict'}")
            except Exception as e:
                logger.error(f"Failed to unpickle document data: {e}")
                raise RuntimeError(f"Failed to deserialize Docling result: {e}")
            
            # ä»å­—å…¸é‡å»º DoclingDocument
            try:
                document = DoclingDocument.model_validate(doc_dict)
                logger.info(f"[MULTIVECTOR] Successfully validated DoclingDocument, pages: {len(document.pages)}")
            except Exception as e:
                logger.error(f"Failed to validate DoclingDocument from dict: {e}")
                raise RuntimeError(f"Failed to rebuild DoclingDocument: {e}")
            
            if not document:
                raise ValueError("Docling parsing returned empty document")
            
            # ğŸ”§ åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ ConversionResult å¯¹è±¡
            # åªåŒ…å«å¿…è¦çš„ document å­—æ®µ
            class SimpleConversionResult:
                def __init__(self, document):
                    self.document = document
            
            result = SimpleConversionResult(document)
            
            logger.info(f"[MULTIVECTOR] Docling parsing completed. Document has {len(document.pages)} pages")
            return result
            
        except Exception as e:
            logger.error(f"Docling parsing failed for {file_path}: {e}")
            raise
        finally:
            # ğŸ”“ é‡Šæ”¾ Metal GPU é”
            release_metal_lock("Docling PDF parsing")
    
    def _save_docling_result(self, file_path: str, result: ConversionResult) -> str:
        """ä¿å­˜doclingè§£æç»“æœåˆ°JSONæ–‡ä»¶"""
        try:
            # ä½¿ç”¨æ•°æ®åº“ç›®å½•çš„docling_cacheå­ç›®å½•
            output_dir = self.docling_cache_dir
            
            # ç”ŸæˆJSONæ–‡ä»¶åï¼ˆç›´æ¥ä½¿ç”¨åŸæ–‡ä»¶åï¼Œä¸æ‹¼æ¥æ—¶é—´æˆ³ï¼‰
            file_stem = Path(file_path).stem
            json_filename = f"{file_stem}.json"
            json_path = output_dir / json_filename
            
            # ä¿å­˜JSONå’Œå›¾ç‰‡æ–‡ä»¶
            result.document.save_as_json(
                filename=json_path,
                indent=2,
                image_mode=ImageRefMode.REFERENCED,  # è‡ªåŠ¨ä¿å­˜å›¾ç‰‡åˆ°artifacts_dir
                artifacts_dir=output_dir
            )
            
            logger.info(f"[MULTIVECTOR] Docling result saved to: {json_path}")
            return str(json_path)
            
        except Exception as e:
            logger.error(f"Failed to save docling result: {e}")
            # è¿”å›ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¿å­˜å¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹
            return ""
    
    def _create_or_update_document(self, file_path: str, file_hash: str, docling_json_path: str) -> Document:
        """åˆ›å»ºæˆ–æ›´æ–°Documentè®°å½•"""
        try:
            from sqlmodel import select
            
            # å°è¯•è·å–ç°æœ‰è®°å½•
            with Session(self.engine) as session:
                stmt = select(Document).where(Document.file_path == file_path)
                document = session.exec(stmt).first()
                
                if document:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    document.file_hash = file_hash
                    document.docling_json_path = docling_json_path
                    document.status = "processing"
                    document.processed_at = datetime.now()
                else:
                    # åˆ›å»ºæ–°è®°å½•
                    document = Document(
                        file_path=file_path,
                        file_hash=file_hash,
                        docling_json_path=docling_json_path,
                        status="processing",
                        processed_at=datetime.now()
                    )
                
                session.add(document)
                session.commit()
                session.refresh(document)
                
                logger.info(f"[MULTIVECTOR] Document record created/updated: ID={document.id}")
                return document
            
        except Exception as e:
            logger.error(f"Failed to create/update document record: {e}")
            raise
    
    def _get_or_create_document_record(self, file_path: str, file_hash: str, docling_json_path: str) -> Document:
        """è·å–æˆ–åˆ›å»ºæ–‡æ¡£è®°å½•ï¼ˆè¾…åŠ©æ–¹æ³•ï¼‰"""
        try:
            return self._create_or_update_document(file_path, file_hash, docling_json_path)
        except Exception as e:
            logger.error(f"Failed to get or create document record: {e}")
            # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªä¸´æ—¶å¯¹è±¡
            return Document(
                file_path=file_path,
                file_hash=file_hash,
                docling_json_path=docling_json_path,
                status="error"
            )
    
    def _generate_chunks(self, document_id: int, docling_doc: DoclingDocument) -> Tuple[List[ParentChunk], List[ChildChunk]]:
        """
        ä½¿ç”¨Docling HybridChunkerè¿›è¡Œæ™ºèƒ½åˆ†å—
        
        è®¾è®¡æ€è·¯ï¼š
        1. ä½¿ç”¨Doclingçš„HybridChunkerè¿›è¡Œè¯­ä¹‰æ„ŸçŸ¥çš„åˆ†å—
        2. chunkerçš„tokenizerä»…ç”¨äºchunkå¤§å°æ§åˆ¶ï¼Œä¸embeddingæ¨¡å‹è§£è€¦
        3. å®é™…embeddingç”Ÿæˆé€šè¿‡APIè°ƒç”¨å®Œæˆï¼Œæ”¯æŒQwen3-Embedding-0.6Bç­‰æ¨¡å‹
        4. é‡‡ç”¨çˆ¶å­å—æ¶æ„ï¼Œçˆ¶å—ç”¨äºæ£€ç´¢ï¼Œå­å—ä¿ç•™ç»†èŠ‚
        
        Args:
            document_id: æ–‡æ¡£ID
            docling_doc: Doclingè§£æçš„æ–‡æ¡£å¯¹è±¡
            
        Returns:
            (parent_chunks, child_chunks): çˆ¶å—å’Œå­å—çš„å…ƒç»„
        """
        logger.info(f"[MULTIVECTOR] Generating chunks using HybridChunker for document ID: {document_id}")
        
        all_parent_chunks = []
        all_child_chunks = []
        
        try:
            # ä½¿ç”¨HybridChunkerç”Ÿæˆchunks
            chunk_iter = self.chunker.chunk(dl_doc=docling_doc)
            chunks = list(chunk_iter)
            
            logger.info(f"[MULTIVECTOR] HybridChunker generated {len(chunks)} chunks")
            
            # ä¸ºæ¯ä¸ªchunkåˆ›å»ºçˆ¶å—å’Œå¯¹åº”çš„å­å—
            for i, chunk in enumerate(chunks):
                try:
                    # è·å–åŸå§‹chunkæ–‡æœ¬ - ç”¨äºçˆ¶å—ï¼ˆä¿æŒæ•°æ®çº¯å‡€æ€§ï¼‰
                    raw_content = chunk.text
                    
                    # è·å–ä¸Šä¸‹æ–‡ä¸°å¯Œçš„å†…å®¹ - ä»…ç”¨äºå­å—æ‘˜è¦ç”Ÿæˆ
                    contextualized_content = self.chunker.contextualize(chunk=chunk)
                    
                    # ç¡®å®šchunkç±»å‹
                    chunk_type = self._determine_chunk_type(chunk)
                    
                    logger.debug(f"[MULTIVECTOR] Chunk {i}: type={chunk_type}, raw_length={len(raw_content)}, contextualized_length={len(contextualized_content)}")
                    
                    # å¦‚æœæ˜¯å›¾ç‰‡ç±»å‹ä¸”åŒ…å«æ··åˆå†…å®¹ï¼Œéœ€è¦è¿›è¡Œåˆ†å‰²å¤„ç†
                    if chunk_type == "image" and self._contains_mixed_content(raw_content):
                        logger.debug(f"[MULTIVECTOR] Chunk {i} contains mixed content, splitting...")
                        # åˆ†å‰²æ··åˆå†…å®¹ä¸ºçº¯å›¾ç‰‡æè¿°å’Œçº¯æ–‡æœ¬éƒ¨åˆ†
                        sub_chunks = self._split_mixed_chunk(chunk, document_id, i)
                        all_parent_chunks.extend([sc[0] for sc in sub_chunks])
                        all_child_chunks.extend([sc[1] for sc in sub_chunks])
                    else:
                        # æ­£å¸¸å¤„ç†å•ä¸€ç±»å‹çš„chunk
                        parent_chunk, child_chunk = self._create_chunk_pair(
                            chunk, document_id, i, chunk_type, raw_content, contextualized_content
                        )
                        all_parent_chunks.append(parent_chunk)
                        all_child_chunks.append(child_chunk)
                    
                except Exception as e:
                    logger.error(f"Failed to process chunk {i}: {e}")
                    continue
            
            logger.info(f"[MULTIVECTOR] Successfully generated {len(all_parent_chunks)} parent chunks and {len(all_child_chunks)} child chunks")
            
        except Exception as e:
            logger.error(f"Failed to generate chunks using HybridChunker: {e}")
            raise
        
        return all_parent_chunks, all_child_chunks
    
    def _determine_chunk_type(self, chunk) -> str:
        """
        æ ¹æ®chunkçš„å†…å®¹å’Œå…ƒæ•°æ®ç¡®å®šç±»å‹
        
        æ³¨æ„ï¼šæˆ‘ä»¬é¿å…ä½¿ç”¨"mixed"ç±»å‹ï¼Œä»¥ä¿æŒæ•°æ®çº¯å‡€æ€§
        å¦‚æœchunkåŒ…å«å¤šç§ç±»å‹ï¼Œæˆ‘ä»¬ä¼šåœ¨_generate_chunksä¸­è¿›è¡Œæ‹†åˆ†å¤„ç†
        
        Args:
            chunk: HybridChunkerç”Ÿæˆçš„chunkå¯¹è±¡
            
        Returns:
            chunkç±»å‹å­—ç¬¦ä¸²: "text", "image", "table"
        """
        try:
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            logger.debug(f"[CHUNK_TYPE] Processing chunk with text length: {len(chunk.text)}")
            logger.debug(f"[CHUNK_TYPE] Chunk has meta: {hasattr(chunk, 'meta')}")
            
            if hasattr(chunk.meta, 'doc_items') and chunk.meta.doc_items:
                logger.debug(f"[CHUNK_TYPE] Found {len(chunk.meta.doc_items)} doc_items")
                
                # æ£€æŸ¥doc_itemsä¸­çš„ç±»å‹
                item_types = set()
                for i, item in enumerate(chunk.meta.doc_items):
                    item_label = None
                    if hasattr(item, 'label'):
                        item_label = str(item.label)
                        item_types.add(item_label)
                    logger.debug(f"[CHUNK_TYPE] Item {i}: type={type(item).__name__}, label={item_label}")
                
                logger.debug(f"[CHUNK_TYPE] All item types found: {item_types}")
                
                # ä¼˜å…ˆçº§ï¼šè¡¨æ ¼ > å›¾ç‰‡ > æ–‡æœ¬
                # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸»è¦å†…å®¹ç±»å‹ä¸è¢«å¿½ç•¥
                if any('TABLE' in item_type.upper() for item_type in item_types):
                    logger.debug("[CHUNK_TYPE] Determined type: table")
                    return "table"
                elif any('PICTURE' in item_type.upper() or 'IMAGE' in item_type.upper() for item_type in item_types):
                    logger.debug("[CHUNK_TYPE] Determined type: image")
                    return "image"
                else:
                    logger.debug("[CHUNK_TYPE] Determined type: text (default)")
                    return "text"
            else:
                logger.debug("[CHUNK_TYPE] No doc_items found, defaulting to text")
                # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œæ ¹æ®å†…å®¹é•¿åº¦å’Œç‰¹å¾åˆ¤æ–­
                return "text"
        except Exception as e:
            logger.warning(f"Failed to determine chunk type: {e}")
            return "text"
    
    def _contains_mixed_content(self, content: str) -> bool:
        """
        æ£€æµ‹å†…å®¹æ˜¯å¦åŒ…å«å›¾ç‰‡æè¿°å’Œæ–‡æœ¬çš„æ··åˆå†…å®¹
        
        Args:
            content: è¦æ£€æµ‹çš„å†…å®¹
            
        Returns:
            bool: æ˜¯å¦åŒ…å«æ··åˆå†…å®¹
        """
        # æ£€æµ‹æ˜¯å¦åŒ…å«å›¾ç‰‡æè¿°çš„ç‰¹å¾
        # å›¾ç‰‡æè¿°é€šå¸¸ä»¥ç‰¹å®šçš„å¥å¼å¼€å¤´ï¼Œå¹¶ä¸”åŒ…å«å›¾ç‰‡ç›¸å…³çš„è¯æ±‡
        content_lower = content.lower()
        
        # å›¾ç‰‡æè¿°çš„å¸¸è§å¼€å¤´å’Œç‰¹å¾è¯
        image_indicators = [
            "the image illustrates",
            "the image shows",
            "the image displays",
            "the diagram shows",
            "the figure illustrates",
            "the chart displays",
            "this image depicts",
            "the visualization shows"
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡æè¿°ç‰¹å¾
        has_image_description = any(indicator in content_lower for indicator in image_indicators)
        
        # æ£€æŸ¥æ˜¯å¦è¿˜åŒ…å«å…¶ä»–æ–‡æœ¬å†…å®¹ï¼ˆé€šå¸¸å›¾ç‰‡æè¿°åä¼šæœ‰æ¢è¡Œå’Œå…¶ä»–æ–‡æœ¬ï¼‰
        lines = content.strip().split('\n')
        has_multiple_sections = len(lines) > 3  # å›¾ç‰‡æè¿°é€šå¸¸æ˜¯1-2è¡Œï¼Œå¦‚æœè¶…è¿‡3è¡Œå¯èƒ½åŒ…å«å…¶ä»–å†…å®¹
        
        # ç®€å•çš„ä¸­æ–‡å†…å®¹æ£€æµ‹ï¼ˆå¦‚æœåŒ…å«å¤§é‡ä¸­æ–‡ï¼Œå¯èƒ½æ˜¯åŸæ–‡ï¼‰
        chinese_chars = len([c for c in content if '\u4e00' <= c <= '\u9fff'])
        has_chinese_content = chinese_chars > 50  # å¦‚æœä¸­æ–‡å­—ç¬¦è¶…è¿‡50ä¸ªï¼Œå¯èƒ½åŒ…å«åŸæ–‡
        
        return has_image_description and (has_multiple_sections or has_chinese_content)
    
    def _split_mixed_chunk(self, chunk, document_id: int, chunk_index: int) -> List[Tuple[ParentChunk, ChildChunk]]:
        """
        åˆ†å‰²åŒ…å«æ··åˆå†…å®¹çš„chunkä¸ºå¤šä¸ªçº¯å‡€çš„chunks
        
        Args:
            chunk: åŸå§‹chunkå¯¹è±¡
            document_id: æ–‡æ¡£ID
            chunk_index: chunkç´¢å¼•
            
        Returns:
            List[Tuple[ParentChunk, ChildChunk]]: åˆ†å‰²åçš„chunkå¯¹åˆ—è¡¨
        """
        result_chunks = []
        content = chunk.text.strip()
        
        try:
            # ç®€å•çš„åˆ†å‰²ç­–ç•¥ï¼šé€šè¿‡æ®µè½åˆ†éš”
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            
            if len(paragraphs) < 2:
                # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„æ®µè½åˆ†éš”ï¼Œå°è¯•æŒ‰å¥å­åˆ†å‰²
                paragraphs = [p.strip() for p in content.split('\n') if p.strip()]
            
            current_section = []
            current_type = None
            
            for para in paragraphs:
                para_lower = para.lower()
                
                # åˆ¤æ–­è¿™ä¸ªæ®µè½æ˜¯å›¾ç‰‡æè¿°è¿˜æ˜¯æ™®é€šæ–‡æœ¬
                is_image_desc = any(indicator in para_lower for indicator in [
                    "the image", "the diagram", "the figure", "the chart", 
                    "image illustrates", "image shows", "image displays"
                ])
                
                para_type = "image" if is_image_desc else "text"
                
                if current_type is None:
                    current_type = para_type
                    current_section.append(para)
                elif current_type == para_type:
                    current_section.append(para)
                else:
                    # ç±»å‹å˜åŒ–ï¼Œä¿å­˜å½“å‰sectionå¹¶å¼€å§‹æ–°çš„section
                    if current_section:
                        section_content = '\n\n'.join(current_section)
                        parent_chunk, child_chunk = self._create_chunk_pair(
                            chunk, document_id, f"{chunk_index}_{current_type}", 
                            current_type, section_content, section_content
                        )
                        result_chunks.append((parent_chunk, child_chunk))
                    
                    # å¼€å§‹æ–°section
                    current_type = para_type
                    current_section = [para]
            
            # å¤„ç†æœ€åä¸€ä¸ªsection
            if current_section:
                section_content = '\n\n'.join(current_section)
                parent_chunk, child_chunk = self._create_chunk_pair(
                    chunk, document_id, f"{chunk_index}_{current_type}", 
                    current_type, section_content, section_content
                )
                result_chunks.append((parent_chunk, child_chunk))
            
            logger.debug(f"[MULTIVECTOR] Split chunk {chunk_index} into {len(result_chunks)} sub-chunks")
            
        except Exception as e:
            logger.error(f"Failed to split mixed chunk {chunk_index}: {e}")
            # å¦‚æœåˆ†å‰²å¤±è´¥ï¼Œè¿”å›åŸå§‹chunkä½œä¸ºtextç±»å‹
            parent_chunk, child_chunk = self._create_chunk_pair(
                chunk, document_id, chunk_index, "text", content, content
            )
            result_chunks = [(parent_chunk, child_chunk)]
        
        return result_chunks
    
    def _create_chunk_pair(self, chunk, document_id: int, chunk_index, chunk_type: str, 
                          raw_content: str, contextualized_content: str) -> Tuple[ParentChunk, ChildChunk]:
        """
        åˆ›å»ºä¸€å¯¹çˆ¶å—å’Œå­å—
        
        Args:
            chunk: åŸå§‹chunkå¯¹è±¡
            document_id: æ–‡æ¡£ID
            chunk_index: chunkç´¢å¼•
            chunk_type: chunkç±»å‹
            raw_content: åŸå§‹å†…å®¹
            contextualized_content: ä¸Šä¸‹æ–‡åŒ–å†…å®¹
            
        Returns:
            Tuple[ParentChunk, ChildChunk]: çˆ¶å—å’Œå­å—çš„å…ƒç»„
        """
        # å‡†å¤‡metadataï¼Œå¯¹äºå›¾ç‰‡ç±»å‹éœ€è¦é¢å¤–ä¿å­˜æ–‡ä»¶è·¯å¾„
        metadata = {
            "chunk_index": chunk_index,
            "original_text_length": len(raw_content),
            "contextualized_length": len(contextualized_content),
            "doc_items_refs": [item.self_ref for item in chunk.meta.doc_items] if hasattr(chunk.meta, 'doc_items') else [],
            "chunk_id": chunk.meta.chunk_id if hasattr(chunk.meta, 'chunk_id') else None,
            "source": "hybrid_chunker_split" if isinstance(chunk_index, str) else "hybrid_chunker"
        }
        
        # å¯¹äºå›¾ç‰‡ç±»å‹ï¼Œå°†æ–‡ä»¶è·¯å¾„ä¿å­˜åˆ°metadataä¸­
        if chunk_type == "image":
            # å°è¯•ä»chunkçš„doc_itemsä¸­æå–å›¾ç‰‡æ–‡ä»¶è·¯å¾„
            image_file_path = self._extract_image_file_path(chunk)
            if image_file_path:
                metadata["image_file_path"] = image_file_path
                logger.debug(f"[MULTIVECTOR] Image chunk {chunk_index}: saved file path to metadata: {image_file_path}")
            else:
                logger.warning(f"[MULTIVECTOR] Image chunk {chunk_index}: could not extract file path")
        
        # åˆ›å»ºçˆ¶å— - contentç»Ÿä¸€å­˜å‚¨"å†…å®¹"ï¼ˆæ–‡æœ¬å†…å®¹æˆ–å›¾ç‰‡æè¿°ï¼‰
        parent_chunk = ParentChunk(
            document_id=document_id,
            chunk_type=chunk_type,
            content=raw_content,  # ç»Ÿä¸€å­˜å‚¨å†…å®¹ï¼šæ–‡æœ¬å†…å®¹æˆ–å›¾ç‰‡æè¿°
            metadata_json=json.dumps(metadata)
        )
        
        # ç”Ÿæˆæ£€ç´¢å‹å¥½çš„å­å—å†…å®¹
        retrieval_content = self._generate_retrieval_summary(contextualized_content, chunk_type)
        
        # åˆ›å»ºå­å—
        child_chunk = ChildChunk(
            parent_chunk_id=0,  # åœ¨å­˜å‚¨æ—¶ä¼šè®¾ç½®æ­£ç¡®çš„ID
            retrieval_content=retrieval_content,
            vector_id=generate_vector_id()
        )
        
        return parent_chunk, child_chunk
    
    def _extract_image_file_path(self, chunk) -> str | None:
        """
        ä»chunkçš„doc_itemsä¸­æå–å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        ä½¿ç”¨doc_items_refsä¸­çš„å›¾ç‰‡ç´¢å¼•æ¥åŒ¹é…doclingç”Ÿæˆçš„å›¾ç‰‡æ–‡ä»¶
        
        Args:
            chunk: HybridChunkerç”Ÿæˆçš„chunkå¯¹è±¡
            
        Returns:
            str | None: å›¾ç‰‡æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        try:
            if not hasattr(chunk.meta, 'doc_items') or not chunk.meta.doc_items:
                return None
            
            # ä»doc_itemsä¸­æå–pictureså¼•ç”¨ç´¢å¼•
            picture_refs = []
            for item in chunk.meta.doc_items:
                if hasattr(item, 'self_ref') and item.self_ref:
                    ref_str = str(item.self_ref)
                    if "/pictures/" in ref_str:
                        # ä¾‹å¦‚: "#/pictures/1" -> æå– "1"
                        try:
                            picture_index = ref_str.split("/pictures/")[-1]
                            picture_refs.append(picture_index)
                        except Exception as e:
                            logger.warning(f"Error extracting picture index from ref: {e}")
                            continue
            
            # æ ¹æ®å›¾ç‰‡ç´¢å¼•æŸ¥æ‰¾å¯¹åº”çš„æ–‡ä»¶
            for picture_index in picture_refs:
                try:
                    # doclingç”Ÿæˆçš„æ–‡ä»¶åæ ¼å¼: image_{6ä½æ•°å­—ç´¢å¼•}_{hash}.png
                    # ä¾‹å¦‚: image_000001_hash.png å¯¹åº” pictures/1
                    padded_index = str(picture_index).zfill(6)
                    pattern = f"image_{padded_index}_*.png"
                    
                    # åœ¨docling_cache_dirä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
                    matching_files = list(self.docling_cache_dir.glob(pattern))
                    
                    if matching_files:
                        # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
                        image_file = matching_files[0]
                        logger.debug(f"Found image file for index {picture_index}: {image_file.name}")
                        return str(image_file)
                    else:
                        logger.debug(f"No image file found for pattern: {pattern}")
                        
                except Exception as e:
                    logger.warning(f"Error processing picture index {picture_index}: {e}")
                    continue
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•åŸæ¥çš„æ–¹æ³•
            for item in chunk.meta.doc_items:
                if hasattr(item, 'label') and 'PICTURE' in str(item.label).upper():
                    # æ£€æŸ¥æ˜¯å¦æœ‰file attribute
                    if hasattr(item, 'file') and item.file:
                        # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
                        if hasattr(item.file, 'filename') and item.file.filename:
                            # å›¾ç‰‡æ–‡ä»¶ä¿å­˜åœ¨docling_cache_dirä¸­
                            image_filename = item.file.filename
                            full_path = self.docling_cache_dir / image_filename
                            if full_path.exists():
                                return str(full_path)
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to extract image file path from chunk: {e}")
            return None
    
    def _generate_retrieval_summary(self, content: str, chunk_type: str) -> str:
        """
        ä½¿ç”¨LLMä¸ºchunkå†…å®¹ç”Ÿæˆæ£€ç´¢å‹å¥½çš„æ‘˜è¦
        
        Args:
            content: å®Œæ•´çš„chunkå†…å®¹
            chunk_type: chunkç±»å‹
            
        Returns:
            æ£€ç´¢ä¼˜åŒ–çš„æ‘˜è¦æ–‡æœ¬
        """
        try:
            # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œç›´æ¥è¿”å›åŸå†…å®¹
            if len(content.strip()) < 50:
                return content.strip()
            
            # ğŸ”§ åœ¨è°ƒç”¨ LLM å‰æ¸…ç† PyTorch Metal çŠ¶æ€
            # ç¡®ä¿ Docling å®Œå…¨é‡Šæ”¾äº† Metal èµ„æº
            try:
                import torch
                import gc
                if torch.backends.mps.is_available():
                    # æ¸…ç† PyTorch MPS ç¼“å­˜
                    torch.mps.empty_cache()
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
            except Exception:
                pass  # å¿½ç•¥æ¸…ç†é”™è¯¯
            
            # æ ¹æ®chunkç±»å‹é€‰æ‹©åˆé€‚çš„æç¤ºè¯
            if chunk_type == "table":
                prompt_prefix = """You are an assistant tasked with summarizing tables for retrieval. 
These summaries will be embedded and used to retrieve the raw table elements. 
Give a concise summary of the table that is well optimized for retrieval.

IMPORTANT: Output ONLY the summary content, without any prefixes like "Here's a summary:", "Summary:", or formatting markers."""
            elif chunk_type == "image":
                prompt_prefix = """You are an assistant tasked with summarizing image content for retrieval. 
These summaries will be embedded and used to retrieve the raw image elements. 
Give a concise summary of the image content that is well optimized for retrieval.

IMPORTANT: Output ONLY the summary content, without any prefixes like "Here's a summary:", "Summary:", or formatting markers."""
            else:
                prompt_prefix = """You are an assistant tasked with summarizing text for retrieval. 
These summaries will be embedded and used to retrieve the raw text elements. 
Give a concise summary of the text that is well optimized for retrieval.

IMPORTANT: Output ONLY the summary content, without any prefixes like "Here's a summary:", "Summary:", or formatting markers."""
            
            # æ„å»ºå®Œæ•´æç¤º
            messages = [
                {"role": "system", "content": prompt_prefix},
                {"role": "user", "content": f"Content to summarize:\n\n{content}"}
            ]
            
            # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
            summary = self.models_mgr.get_chat_completion(messages)
            
            if summary and summary.strip():
                # åå¤„ç†ï¼šæ¸…ç†å¸¸è§çš„æ ¼å¼åŒ–å‰ç¼€
                cleaned_summary = self._clean_summary_prefixes(summary.strip())
                return cleaned_summary
            else:
                # å¦‚æœLLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ–‡æœ¬æˆªå–
                return content[:500] + "..." if len(content) > 500 else content
                
        except Exception as e:
            logger.error(f"Failed to generate retrieval summary: {e}")
            # é™çº§å¤„ç†ï¼šç›´æ¥æˆªå–å†…å®¹
            return content[:500] + "..." if len(content) > 500 else content
    
    def _clean_summary_prefixes(self, summary: str) -> str:
        """
        æ¸…ç†LLMç”Ÿæˆæ‘˜è¦ä¸­çš„å¸¸è§æ ¼å¼åŒ–å‰ç¼€
        
        Args:
            summary: åŸå§‹æ‘˜è¦æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ‘˜è¦æ–‡æœ¬
        """
        # å¸¸è§çš„éœ€è¦æ¸…ç†çš„å‰ç¼€æ¨¡å¼
        prefixes_to_remove = [
            "Here's a retrieval-optimized summary:",
            "Here's a retrieval-optimized summary of the text:",
            "Here's a retrieval-optimized summary of the image:",
            "Here's a retrieval-optimized summary of the table:",
            "Here's a concise summary:",
            "**Summary:**",
            "Summary:",
            "**Image Summary:**",
            "Image Summary:",
            "**Table Summary:**",
            "Table Summary:",
            "The image",
            "The table",
            "This text"
        ]
        
        cleaned = summary.strip()
        
        # ç§»é™¤å¼€å¤´çš„æ ¼å¼åŒ–å‰ç¼€
        for prefix in prefixes_to_remove:
            if cleaned.lower().startswith(prefix.lower()):
                cleaned = cleaned[len(prefix):].strip()
                break
        
        # ç§»é™¤å¼€å¤´çš„å†’å·ã€ç ´æŠ˜å·ç­‰
        while cleaned and cleaned[0] in [':', '-', '*', ' ']:
            cleaned = cleaned[1:].strip()
        
        return cleaned if cleaned else summary  # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè¿”å›åŸå§‹å†…å®¹
    
    def _create_image_context_chunks(self, parent_chunks: List[ParentChunk], child_chunks: List[ChildChunk], 
                                   document_id: int) -> Tuple[List[ParentChunk], List[ChildChunk]]:
        """
        ä¸ºå›¾åƒå—åˆ›å»ºé¢å¤–çš„ä¸Šä¸‹æ–‡å—ï¼ˆå›¾ç‰‡æè¿° + å‘¨å›´åŸå§‹æ–‡æœ¬çš„æ‘˜è¦ï¼‰
        è¿™ä¸ªåŠŸèƒ½å®ç°äº†MULTIVECTOR.mdä¸­çš„å…³é”®è®¾è®¡ï¼šåŠ å¼ºå›¾åƒä¸æ–‡æœ¬çš„å…³è”æ£€ç´¢
        
        Args:
            parent_chunks: ç°æœ‰çš„çˆ¶å—åˆ—è¡¨
            child_chunks: ç°æœ‰çš„å­å—åˆ—è¡¨
            document_id: æ–‡æ¡£ID
            
        Returns:
            Tuple[List[ParentChunk], List[ChildChunk]]: æ›´æ–°åçš„çˆ¶å—å’Œå­å—åˆ—è¡¨
        """
        try:
            additional_parent_chunks = []
            additional_child_chunks = []
            
            # æ‰¾åˆ°æ‰€æœ‰å›¾åƒå—
            image_chunks = [(i, chunk) for i, chunk in enumerate(parent_chunks) if chunk.chunk_type == "image"]
            
            if not image_chunks:
                return parent_chunks, child_chunks
            
            logger.info(f"Found {len(image_chunks)} image chunks, creating context chunks...")
            
            for chunk_idx, image_chunk in image_chunks:
                try:
                    # è·å–å›¾åƒæè¿°å†…å®¹ - ç°åœ¨ç›´æ¥ä»contentå­—æ®µè·å–
                    image_description = image_chunk.content
                    
                    if not image_description or len(image_description.strip()) < 10:
                        logger.warning(f"Image chunk {chunk_idx}: description too short or empty")
                        continue
                    
                    # è·å–å‘¨å›´çš„æ–‡æœ¬å—å†…å®¹è¿›è¡Œæ‘˜è¦
                    surrounding_texts = self._get_surrounding_text_chunks(parent_chunks, chunk_idx)
                    if not surrounding_texts:
                        continue
                    
                    # ç”Ÿæˆå‘¨å›´æ–‡æœ¬çš„æ‘˜è¦
                    context_summary = self._generate_context_summary(surrounding_texts)
                    if not context_summary:
                        continue
                    
                    # åˆ›å»ºç»„åˆå†…å®¹ï¼šå›¾ç‰‡æè¿° + å‘¨å›´æ–‡æœ¬æ‘˜è¦
                    combined_content = f"å›¾åƒå†…å®¹ï¼š{image_description}\n\nç›¸å…³æ–‡æœ¬èƒŒæ™¯ï¼š{context_summary}"
                    
                    # åˆ›å»ºé¢å¤–çš„çˆ¶å—ï¼ˆå›¾åƒä¸Šä¸‹æ–‡å—ï¼‰
                    context_parent = ParentChunk(
                        document_id=document_id,
                        chunk_type="image_context",
                        content=combined_content,
                        metadata_json=json.dumps({
                            "related_image_chunk_index": chunk_idx,
                            "context_type": "image_with_text_summary",
                            "source": "multivector_image_context",
                            "surrounding_chunks_count": len(surrounding_texts)
                        })
                    )
                    
                    # ç”Ÿæˆæ£€ç´¢å‹å¥½çš„å­å—å†…å®¹
                    retrieval_content = self._generate_retrieval_summary(combined_content, "image_context")
                    
                    # åˆ›å»ºå¯¹åº”çš„å­å—
                    context_child = ChildChunk(
                        parent_chunk_id=0,  # åœ¨å­˜å‚¨æ—¶ä¼šè®¾ç½®æ­£ç¡®çš„ID
                        retrieval_content=retrieval_content,
                        vector_id=generate_vector_id()
                    )
                    
                    additional_parent_chunks.append(context_parent)
                    additional_child_chunks.append(context_child)
                    
                    logger.debug(f"Created image context chunk for image chunk {chunk_idx}")
                    
                except Exception as e:
                    logger.error(f"Failed to create context chunk for image chunk {chunk_idx}: {e}")
                    continue
            
            # åˆå¹¶æ‰€æœ‰å—
            all_parent_chunks = parent_chunks + additional_parent_chunks
            all_child_chunks = child_chunks + additional_child_chunks
            
            logger.info(f"Created {len(additional_parent_chunks)} additional image context chunks")
            return all_parent_chunks, all_child_chunks
            
        except Exception as e:
            logger.error(f"Failed to create image context chunks: {e}")
            return parent_chunks, child_chunks
    
    def _get_surrounding_text_chunks(self, parent_chunks: List[ParentChunk], image_chunk_idx: int, 
                                   context_window: int = 2) -> List[str]:
        """
        è·å–å›¾åƒå—å‘¨å›´çš„æ–‡æœ¬å—å†…å®¹
        
        Args:
            parent_chunks: æ‰€æœ‰çˆ¶å—åˆ—è¡¨
            image_chunk_idx: å›¾åƒå—çš„ç´¢å¼•
            context_window: å‰åæ–‡æœ¬å—çš„æ•°é‡
            
        Returns:
            List[str]: å‘¨å›´æ–‡æœ¬å—çš„å†…å®¹åˆ—è¡¨
        """
        surrounding_texts = []
        
        # è·å–å‰é¢çš„æ–‡æœ¬å—
        start_idx = max(0, image_chunk_idx - context_window)
        for i in range(start_idx, image_chunk_idx):
            chunk = parent_chunks[i]
            if chunk.chunk_type == "text" and chunk.content:
                surrounding_texts.append(chunk.content)
        
        # è·å–åé¢çš„æ–‡æœ¬å—
        end_idx = min(len(parent_chunks), image_chunk_idx + context_window + 1)
        for i in range(image_chunk_idx + 1, end_idx):
            chunk = parent_chunks[i]
            if chunk.chunk_type == "text" and chunk.content:
                surrounding_texts.append(chunk.content)
        
        return surrounding_texts
    
    def _generate_context_summary(self, text_chunks: List[str]) -> str:
        """
        ä¸ºå‘¨å›´çš„æ–‡æœ¬å—ç”Ÿæˆæ‘˜è¦
        
        Args:
            text_chunks: æ–‡æœ¬å—å†…å®¹åˆ—è¡¨
            
        Returns:
            str: ç”Ÿæˆçš„æ‘˜è¦
        """
        if not text_chunks:
            return ""
        
        try:
            # åˆå¹¶æ–‡æœ¬å†…å®¹
            combined_text = "\n\n".join(text_chunks)
            
            # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
            if len(combined_text.strip()) < 50:
                return combined_text.strip()
            
            # æ„å»ºæ‘˜è¦æç¤º
            prompt = """ä½ éœ€è¦ä¸ºä»¥ä¸‹æ–‡æœ¬å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œè¿™ä¸ªæ‘˜è¦å°†ä¸å›¾åƒæè¿°ç»„åˆï¼Œç”¨äºå¢å¼ºå›¾åƒä¸æ–‡æœ¬çš„å…³è”æ£€ç´¢ã€‚

è¯·ç”Ÿæˆä¸€ä¸ªçªå‡ºä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯çš„æ‘˜è¦ï¼Œå¸®åŠ©ç†è§£å›¾åƒåœ¨æ–‡æ¡£ä¸­çš„ä¸Šä¸‹æ–‡èƒŒæ™¯ã€‚

é‡è¦ï¼šåªè¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«"æ‘˜è¦ï¼š"ã€"æ€»ç»“ï¼š"ç­‰å‰ç¼€ã€‚

æ–‡æœ¬å†…å®¹ï¼š
"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ‘˜è¦åŠ©æ‰‹ï¼Œæ“…é•¿ç”Ÿæˆç®€æ´å‡†ç¡®çš„æ‘˜è¦ã€‚"},
                {"role": "user", "content": f"{prompt}\n\n{combined_text}"}
            ]
            
            # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
            summary = self.models_mgr.get_chat_completion(messages)
            
            if summary and summary.strip():
                # æ¸…ç†æ ¼å¼åŒ–å‰ç¼€
                cleaned_summary = self._clean_summary_prefixes(summary.strip())
                return cleaned_summary
            else:
                # å¦‚æœLLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æˆªå–
                return combined_text[:300] + "..." if len(combined_text) > 300 else combined_text
                
        except Exception as e:
            logger.error(f"Failed to generate context summary: {e}")
            # é™çº§å¤„ç†ï¼šæˆªå–å‰é¢éƒ¨åˆ†å†…å®¹
            combined_text = "\n\n".join(text_chunks)
            return combined_text[:300] + "..." if len(combined_text) > 300 else combined_text
    
    def _store_chunks(self, parent_chunks: List[ParentChunk], child_chunks: List[ChildChunk]):
        """å­˜å‚¨çˆ¶å—å’Œå­å—åˆ°SQLite"""
        if len(parent_chunks) != len(child_chunks):
            raise ValueError(f"Parent chunks count ({len(parent_chunks)}) does not match child chunks count ({len(child_chunks)})")
        
        # 1. å…ˆå­˜å‚¨çˆ¶å—
        if parent_chunks:
            with Session(self.engine) as session:
                session.add_all(parent_chunks)
                session.commit()
                # åˆ·æ–°ä»¥è·å–ç”Ÿæˆçš„ID
                for chunk in parent_chunks:
                    session.refresh(chunk)
            
            logger.info(f"[MULTIVECTOR] Stored {len(parent_chunks)} parent chunks")
        
        # 2. è®¾ç½®å­å—çš„parent_chunk_idå¹¶å­˜å‚¨
        if child_chunks:
            for i, child_chunk in enumerate(child_chunks):
                child_chunk.parent_chunk_id = parent_chunks[i].id
            with Session(self.engine) as session:
                session.add_all(child_chunks)
                session.commit()
                
                # åˆ·æ–°ä»¥è·å–ç”Ÿæˆçš„ID
                for chunk in child_chunks:
                    session.refresh(chunk)
            
            logger.info(f"[MULTIVECTOR] Stored {len(child_chunks)} child chunks")
    
    def _vectorize_and_store(self, parent_chunks: List[ParentChunk], child_chunks: List[ChildChunk]):
        """å‘é‡åŒ–å­å—å¹¶å­˜å‚¨åˆ°LanceDBï¼ˆçˆ¶å—ä¸éœ€è¦å‘é‡åŒ–ï¼‰"""
        try:
            # ç¡®ä¿vectorsè¡¨å·²åˆå§‹åŒ–
            self.lancedb_mgr.init_vectors_table()
            
            vector_records = []
            
            # åªå¤„ç†å­å—å‘é‡åŒ–ï¼ˆçˆ¶å—ä¸éœ€è¦å‘é‡åŒ–ï¼Œå®ƒä»¬æ˜¯ç”¨äºç­”æ¡ˆåˆæˆçš„åŸå§‹å†…å®¹ï¼‰
            for i, child_chunk in enumerate(child_chunks):
                try:
                    # è°ƒç”¨embeddingæ¨¡å‹è¿›è¡Œå‘é‡åŒ–
                    embedding = self.models_mgr.get_embedding(child_chunk.retrieval_content)
                    
                    if not embedding:
                        logger.warning(f"Failed to get embedding for child chunk ID: {child_chunk.id}")
                        continue
                    
                    # ä»å¯¹åº”çš„çˆ¶å—è·å–document_id
                    parent_chunk = parent_chunks[i] if i < len(parent_chunks) else None
                    document_id = parent_chunk.document_id if parent_chunk else 0
                    
                    # åˆ›å»ºå­å—å‘é‡è®°å½•
                    vector_record = {
                        "vector_id": child_chunk.vector_id,
                        "vector": embedding,
                        "parent_chunk_id": child_chunk.parent_chunk_id,
                        "document_id": document_id,
                        "retrieval_content": child_chunk.retrieval_content[:500]  # å­˜å‚¨å‰500å­—ç¬¦ç”¨äºæ£€ç´¢æ˜¾ç¤º
                    }
                    vector_records.append(vector_record)
                    
                except Exception as e:
                    logger.error(f"Failed to vectorize child chunk ID {child_chunk.id}: {e}")
                    continue
            
            # æ‰¹é‡å­˜å‚¨åˆ°LanceDB
            if vector_records:
                self.lancedb_mgr.add_vectors(vector_records)
                logger.info(f"[MULTIVECTOR] Vectorized and stored {len(vector_records)} child chunk vectors")
            
            logger.info(f"[MULTIVECTOR] Vector storage completed - {len(parent_chunks)} parent chunks stored in SQLite only, {len(child_chunks)} child chunks vectorized in LanceDB")
            
        except Exception as e:
            logger.error(f"Failed to vectorize and store: {e}")
            raise


# ä¸ºäº†æµ‹è¯•å’Œè°ƒè¯•ä½¿ç”¨
def test_multivector_file():
    """æµ‹è¯•åˆ†å—ç®¡ç†å™¨çš„åŸºæœ¬åŠŸèƒ½"""
    # 1. åˆå§‹åŒ–å„ä¸ªç»„ä»¶
    logger.info("ğŸ”§ åˆå§‹åŒ–ç»„ä»¶...")
    # SQLiteæ•°æ®åº“
    from core.config import TEST_DB_PATH
    from sqlmodel import create_engine
    engine = create_engine(f'sqlite:///{TEST_DB_PATH}')
    # LanceDB
    db_directory = Path(TEST_DB_PATH).parent
    lancedb_mgr = LanceDBMgr(base_dir=db_directory)
    # æ¨¡å‹ç®¡ç†å™¨
    models_mgr = ModelsMgr(engine, base_dir=db_directory)
    # åˆ†å—ç®¡ç†å™¨
    try:
        multivector_mgr = MultiVectorMgr(engine, lancedb_mgr, models_mgr)
        logging.info('âœ… MultivectorMgråˆå§‹åŒ–æˆåŠŸ')
        logging.info('âœ… Tokenizerè§£è€¦æ¶æ„å·²å¯ç”¨')
        logging.info(f'âœ… Chunkeræœ€å¤§tokens: {multivector_mgr.chunker.tokenizer.get_max_tokens()}')
    except Exception as e:
        logger.info(f"âŒ Doclingè½¬æ¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    logger.info("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    # 2. æ‰¾ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£
    # file_path = "/Users/dio/Downloads/Context Engineering for AI Agents_ Lessons from Building Manus.pdf"
    file_path = "/Users/dio/Downloads/AIä»£ç†çš„ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼šæ„å»ºManusçš„ç»éªŒæ•™è®­.pdf"
    
    # # 3. ä»process_document()ä¸­æ‹†åˆ†å‡ºçš„æ–¹æ³•è¿›è¡Œç‹¬ç«‹æµ‹è¯•
    # logger.info("ğŸ§ª æµ‹è¯•åŸºæœ¬æ–¹æ³•...")
    # file_hash = multivector_mgr._calculate_file_hash(file_path)
    # logger.info(f"âœ… æ–‡ä»¶å“ˆå¸Œè®¡ç®—å®Œæˆ: {file_hash}")
    # existing_doc = multivector_mgr._get_existing_document(file_path, file_hash)
    # if existing_doc:
    #     logger.info(f"âœ… å·²å­˜åœ¨æ–‡æ¡£è®°å½•: {existing_doc.id}, çŠ¶æ€: {existing_doc.status}")
    # else:
    #     logger.info("âœ… æœªæ‰¾åˆ°ç°æœ‰æ–‡æ¡£è®°å½•")
    # docling_result = multivector_mgr._parse_with_docling(file_path)
    # logger.info(f"âœ… Doclingè§£æå®Œæˆ: {len(docling_result.document.pages)}é¡µ")
    # docling_json_path = multivector_mgr._save_docling_result(file_path, docling_result)
    # logger.info(f"âœ… Doclingç»“æœä¿å­˜å®Œæˆ: {docling_json_path}")
    # document = multivector_mgr._create_or_update_document(file_path, file_hash, docling_json_path)
    # logger.info(f"âœ… æ–‡æ¡£è®°å½•åˆ›å»º/æ›´æ–°å®Œæˆ, ID: {document.id}")
    # parent_chunks, child_chunks = multivector_mgr._generate_chunks(document.id, docling_result.document)
    # logger.info(f"âœ… ç”Ÿæˆå†…å®¹å—å®Œæˆ: {len(parent_chunks)}çˆ¶å—, {len(child_chunks)}å­å—")
    # multivector_mgr._store_chunks(parent_chunks, child_chunks)        
    # multivector_mgr._vectorize_and_store(parent_chunks, child_chunks)
    # document.status = "done"
    # document.processed_at = datetime.now()
    # multivector_mgr.session.add(document)
    # multivector_mgr.session.commit()

    # 4. æœ€åé›†æˆæµ‹è¯•æ–‡æ¡£å¤„ç†
    try:
        # å¤„ç†æ–‡æ¡£
        result = multivector_mgr.process_document(str(file_path))
        logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {result}")
    
    except Exception as e:
        logger.info(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è®¾ç½®æµ‹è¯•æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    # # 10ç§’å€’è®¡æ—¶
    # import time
    # for i in range(10, 0, -1):
    #     print(f"å€’è®¡æ—¶: {i}ç§’")
    #     time.sleep(1)
    
    test_multivector_file()
