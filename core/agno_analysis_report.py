#!/usr/bin/env python3
"""
agnoåº“æ¶æ„åˆ†æä¸Pythonå®ç°æ¡ˆä¾‹
æœ¬æ–‡ä»¶è¯¦ç»†åˆ†æagnoä¸‰æ–¹åº“çš„æ¶æ„ã€é›†æˆæ–¹å¼ã€å¤šç”¨æˆ·æ”¯æŒã€æµå¼è¾“å‡ºç­‰ç‰¹æ€§
å¹¶æä¾›å®Œæ•´çš„Pythonå®ç°æ¡ˆä¾‹
"""

import asyncio
import json
import time
from typing import Any, Dict, List, Optional, Union
from uuid import uuid4

import openai
from pydantic import BaseModel, Field


class AgnoArchitectureAnalysis:
    """agnoåº“æ¶æ„åˆ†æç±»"""

    def __init__(self):
        self.title = "agnoä¸‰æ–¹åº“æ¶æ„æ·±åº¦åˆ†æ"
        self.description = "åˆ†æOpenAIæ¨¡å‹é›†æˆã€å¤šç”¨æˆ·ä¼šè¯ã€æµå¼è¾“å‡ºç­‰æ ¸å¿ƒç‰¹æ€§"

    def analyze_model_integration(self) -> Dict[str, Any]:
        """
        åˆ†æOpenAIæ¨¡å‹é›†æˆæ–¹å¼

        æ ¸å¿ƒå‘ç°ï¼š
        1. æ”¯æŒæ ‡å‡†OpenAI APIå‚æ•°ï¼šapi_key, base_url, model_id
        2. æä¾›çµæ´»çš„å®¢æˆ·ç«¯é…ç½®ï¼štimeout, max_retries, headersç­‰
        3. åŒæ­¥å’Œå¼‚æ­¥åŒæ¨¡å¼æ”¯æŒ
        4. æµå¼å’Œéæµå¼å“åº”å¤„ç†
        """
        return {
            "æ¨¡å‹é›†æˆæ¶æ„": {
                "æ ¸å¿ƒç±»": "OpenAIChat",
                "å…³é”®å‚æ•°": {
                    "api_key": "APIå¯†é’¥ï¼Œæ”¯æŒç¯å¢ƒå˜é‡å’Œç›´æ¥è®¾ç½®",
                    "base_url": "è‡ªå®šä¹‰APIç«¯ç‚¹ï¼Œæ”¯æŒOpenAIå…¼å®¹æ¥å£",
                    "id": "æ¨¡å‹IDï¼Œå¦‚gpt-4o, gpt-3.5-turboç­‰",
                    "organization": "OpenAIç»„ç»‡IDï¼ˆå¯é€‰ï¼‰",
                    "timeout": "è¯·æ±‚è¶…æ—¶æ—¶é—´",
                    "max_retries": "æœ€å¤§é‡è¯•æ¬¡æ•°",
                    "default_headers": "è‡ªå®šä¹‰HTTPå¤´",
                    "http_client": "è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯"
                },
                "æ”¯æŒçš„åŠŸèƒ½": [
                    "æ–‡æœ¬ç”Ÿæˆ",
                    "æµå¼è¾“å‡º",
                    "å·¥å…·è°ƒç”¨",
                    "å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾ç‰‡ã€éŸ³é¢‘ã€æ–‡ä»¶ï¼‰",
                    "ç»“æ„åŒ–è¾“å‡º",
                    "å¼‚æ­¥å¤„ç†"
                ]
            }
        }

    def analyze_multi_user_sessions(self) -> Dict[str, Any]:
        """
        åˆ†æå¤šç”¨æˆ·ä¼šè¯å¹¶è¡Œæ”¯æŒ

        æ ¸å¿ƒå‘ç°ï¼š
        1. åŸºäºsession_idçš„ä¼šè¯éš”ç¦»
        2. æ”¯æŒuser_idå¤šç”¨æˆ·åŒºåˆ†
        3. å¼‚æ­¥å¹¶å‘å¤„ç†èƒ½åŠ›
        4. ä¼šè¯çŠ¶æ€æŒä¹…åŒ–
        """
        return {
            "å¤šç”¨æˆ·ä¼šè¯æ¶æ„": {
                "ä¼šè¯ç®¡ç†": {
                    "session_id": "å”¯ä¸€ä¼šè¯æ ‡è¯†ç¬¦",
                    "user_id": "ç”¨æˆ·æ ‡è¯†ç¬¦ï¼Œæ”¯æŒå¤šç”¨æˆ·éš”ç¦»",
                    "agent_id": "æ™ºèƒ½ä½“ID",
                    "team_id": "å›¢é˜ŸIDï¼ˆåä½œåœºæ™¯ï¼‰",
                    "workflow_id": "å·¥ä½œæµID"
                },
                "å¹¶å‘æ”¯æŒ": {
                    "å¼‚æ­¥å¤„ç†": "æ”¯æŒasyncioå¹¶å‘",
                    "ä¼šè¯éš”ç¦»": "æ¯ä¸ªsession_idç‹¬ç«‹å¤„ç†",
                    "èµ„æºç®¡ç†": "å†…ç½®è¿æ¥æ± å’Œèµ„æºé™åˆ¶",
                    "çŠ¶æ€ç®¡ç†": "æ”¯æŒä¼šè¯çŠ¶æ€æŒä¹…åŒ–"
                },
                "ä¼šè¯åŠŸèƒ½": [
                    "æ¶ˆæ¯å†å²ç®¡ç†",
                    "ä¼šè¯æ‘˜è¦ç”Ÿæˆ",
                    "è·¨ä¼šè¯è®°å¿†",
                    "å·¥å…·è°ƒç”¨å†å²",
                    "è¿è¡ŒçŠ¶æ€è·Ÿè¸ª"
                ]
            }
        }

    def analyze_streaming_output(self) -> Dict[str, Any]:
        """
        åˆ†ææµå¼å¯¹è¯è¾“å‡ºæœºåˆ¶

        æ ¸å¿ƒå‘ç°ï¼š
        1. åŸºäºgeneratorçš„æµå¼è¾“å‡º
        2. æ”¯æŒå¤–éƒ¨æµå¼åé¦ˆ
        3. å®æ—¶äº‹ä»¶å¤„ç†
        4. å¤šç§æµå¼æ ¼å¼æ”¯æŒ
        """
        return {
            "æµå¼è¾“å‡ºæ¶æ„": {
                "æµå¼æœºåˆ¶": {
                    "åŒæ­¥æµå¼": "Iterator[ModelResponse]",
                    "å¼‚æ­¥æµå¼": "AsyncIterator[ModelResponse]",
                    "äº‹ä»¶é©±åŠ¨": "åŸºäºModelResponseEvent",
                    "å¢é‡æ›´æ–°": "æ”¯æŒå†…å®¹å¢é‡æ›´æ–°"
                },
                "å¤–éƒ¨åé¦ˆ": {
                    "WebSocket": "å®æ—¶åŒå‘é€šä¿¡",
                    "SSE": "Server-Sent Events",
                    "HTTPæµå¼": "æ ‡å‡†HTTPæµå¼å“åº”",
                    "è‡ªå®šä¹‰åè®®": "æ”¯æŒè‡ªå®šä¹‰æµå¼åè®®"
                },
                "æµå¼å†…å®¹": [
                    "æ–‡æœ¬å†…å®¹æµ",
                    "å·¥å…·è°ƒç”¨æµ",
                    "å¤šæ¨¡æ€å†…å®¹æµ",
                    "é”™è¯¯ä¿¡æ¯æµ",
                    "çŠ¶æ€æ›´æ–°æµ"
                ]
            }
        }

    def analyze_input_output_formats(self) -> Dict[str, Any]:
        """
        åˆ†æè¾“å…¥è¾“å‡ºæ ¼å¼è¦æ±‚

        æ ¸å¿ƒå‘ç°ï¼š
        1. æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼šæ–‡æœ¬ã€å›¾ç‰‡ã€éŸ³é¢‘ã€æ–‡ä»¶
        2. çµæ´»çš„è¾“å‡ºæ ¼å¼ï¼šç»“æ„åŒ–ã€æµå¼ã€å¤šæ¨¡æ€
        3. å®Œæ•´çš„æ¶ˆæ¯å…ƒæ•°æ®æ”¯æŒ
        """
        return {
            "è¾“å…¥æ ¼å¼è¦æ±‚": {
                "æ¶ˆæ¯ç»“æ„": {
                    "role": "system/user/assistant/tool",
                    "content": "æ–‡æœ¬å†…å®¹æˆ–ç»“æ„åŒ–å†…å®¹",
                    "name": "å¯é€‰çš„æ¶ˆæ¯åç§°",
                    "tool_calls": "å·¥å…·è°ƒç”¨ä¿¡æ¯",
                    "tool_call_id": "å·¥å…·è°ƒç”¨ID"
                },
                "å¤šæ¨¡æ€æ”¯æŒ": {
                    "images": "å›¾ç‰‡è¾“å…¥ï¼ˆURLã€base64ã€æ–‡ä»¶ï¼‰",
                    "audio": "éŸ³é¢‘è¾“å…¥ï¼ˆå¤šç§æ ¼å¼ï¼‰",
                    "videos": "è§†é¢‘è¾“å…¥",
                    "files": "æ–‡æ¡£æ–‡ä»¶è¾“å…¥"
                },
                "é«˜çº§åŠŸèƒ½": {
                    "å·¥å…·è°ƒç”¨": "function callingæ”¯æŒ",
                    "ç»“æ„åŒ–è¾“å‡º": "Pydanticæ¨¡å‹é›†æˆ",
                    "æµå¼è¾“å…¥": "æ”¯æŒæµå¼è¾“å…¥å¤„ç†",
                    "å¼•ç”¨æ”¯æŒ": "æ–‡æ¡£å¼•ç”¨å’ŒURLå¼•ç”¨"
                }
            },
            "è¾“å‡ºæ ¼å¼è§„èŒƒ": {
                "å“åº”ç»“æ„": {
                    "content": "ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹",
                    "role": "å“åº”è§’è‰²",
                    "tool_calls": "å·¥å…·è°ƒç”¨ç»“æœ",
                    "reasoning_content": "æ¨ç†è¿‡ç¨‹å†…å®¹",
                    "audio_output": "éŸ³é¢‘è¾“å‡º",
                    "metrics": "ä½¿ç”¨æŒ‡æ ‡"
                },
                "å…ƒæ•°æ®": {
                    "usage": "tokenä½¿ç”¨ç»Ÿè®¡",
                    "timing": "å“åº”æ—¶é—´æŒ‡æ ‡",
                    "model": "ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯",
                    "citations": "å¼•ç”¨ä¿¡æ¯"
                }
            }
        }


class OpenAIModelIntegration:
    """OpenAIæ¨¡å‹é›†æˆå®ç°æ¡ˆä¾‹"""

    def __init__(self, api_key: str, base_url: str = None, model_id: str = "gpt-3.5-turbo"):
        """
        åˆå§‹åŒ–OpenAIæ¨¡å‹é›†æˆ

        Args:
            api_key: OpenAI APIå¯†é’¥
            base_url: è‡ªå®šä¹‰APIç«¯ç‚¹
            model_id: æ¨¡å‹ID
        """
        self.api_key = api_key
        self.base_url = base_url
        self.model_id = model_id
        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        self.async_client = openai.AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )

    def create_chat_completion(self, messages: List[Dict[str, Any]], **kwargs) -> Dict[str, Any]:
        """
        åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            APIå“åº”ç»“æœ
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model_id,
                messages=messages,
                **kwargs
            )
            return {
                "content": response.choices[0].message.content,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "model": response.model,
                "finish_reason": response.choices[0].finish_reason
            }
        except Exception as e:
            return {"error": str(e)}

    def create_streaming_completion(self, messages: List[Dict[str, Any]], **kwargs):
        """
        åˆ›å»ºæµå¼èŠå¤©å®Œæˆè¯·æ±‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            æµå¼å“åº”ç‰‡æ®µ
        """
        try:
            stream = self.client.chat.completions.create(
                model=self.model_id,
                messages=messages,
                stream=True,
                **kwargs
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield {
                        "content": chunk.choices[0].delta.content,
                        "finish_reason": chunk.choices[0].finish_reason,
                        "usage": chunk.usage.model_dump() if chunk.usage else None
                    }
        except Exception as e:
            yield {"error": str(e)}

    async def create_async_completion(self, messages: List[Dict[str, Any]], **kwargs) -> Dict[str, Any]:
        """
        åˆ›å»ºå¼‚æ­¥èŠå¤©å®Œæˆè¯·æ±‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            APIå“åº”ç»“æœ
        """
        try:
            response = await self.async_client.chat.completions.create(
                model=self.model_id,
                messages=messages,
                **kwargs
            )
            return {
                "content": response.choices[0].message.content,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "model": response.model,
                "finish_reason": response.choices[0].finish_reason
            }
        except Exception as e:
            return {"error": str(e)}

    async def create_async_streaming_completion(self, messages: List[Dict[str, Any]], **kwargs):
        """
        åˆ›å»ºå¼‚æ­¥æµå¼èŠå¤©å®Œæˆè¯·æ±‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            å¼‚æ­¥æµå¼å“åº”ç‰‡æ®µ
        """
        try:
            stream = await self.async_client.chat.completions.create(
                model=self.model_id,
                messages=messages,
                stream=True,
                **kwargs
            )

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield {
                        "content": chunk.choices[0].delta.content,
                        "finish_reason": chunk.choices[0].finish_reason,
                        "usage": chunk.usage.model_dump() if chunk.usage else None
                    }
        except Exception as e:
            yield {"error": str(e)}


class MultiUserSessionManager:
    """å¤šç”¨æˆ·ä¼šè¯ç®¡ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–ä¼šè¯ç®¡ç†å™¨"""
        self.sessions: Dict[str, Dict[str, Any]] = {}
        self.user_sessions: Dict[str, List[str]] = {}

    def create_session(self, user_id: str, agent_id: str = None) -> str:
        """
        åˆ›å»ºæ–°ä¼šè¯

        Args:
            user_id: ç”¨æˆ·ID
            agent_id: æ™ºèƒ½ä½“ID

        Returns:
            ä¼šè¯ID
        """
        session_id = str(uuid4())

        session = {
            "session_id": session_id,
            "user_id": user_id,
            "agent_id": agent_id,
            "created_at": int(time.time()),
            "updated_at": int(time.time()),
            "messages": [],
            "status": "active",
            "metadata": {}
        }

        self.sessions[session_id] = session

        if user_id not in self.user_sessions:
            self.user_sessions[user_id] = []
        self.user_sessions[user_id].append(session_id)

        return session_id

    def get_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–ä¼šè¯ä¿¡æ¯

        Args:
            session_id: ä¼šè¯ID

        Returns:
            ä¼šè¯ä¿¡æ¯
        """
        return self.sessions.get(session_id)

    def add_message(self, session_id: str, role: str, content: str, **kwargs) -> bool:
        """
        æ·»åŠ æ¶ˆæ¯åˆ°ä¼šè¯

        Args:
            session_id: ä¼šè¯ID
            role: æ¶ˆæ¯è§’è‰²
            content: æ¶ˆæ¯å†…å®¹
            **kwargs: å…¶ä»–æ¶ˆæ¯å±æ€§

        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        if session_id not in self.sessions:
            return False

        message = {
            "id": str(uuid4()),
            "role": role,
            "content": content,
            "created_at": int(time.time()),
            **kwargs
        }

        self.sessions[session_id]["messages"].append(message)
        self.sessions[session_id]["updated_at"] = int(time.time())

        return True

    def get_session_messages(self, session_id: str, limit: int = None) -> List[Dict[str, Any]]:
        """
        è·å–ä¼šè¯æ¶ˆæ¯

        Args:
            session_id: ä¼šè¯ID
            limit: æ¶ˆæ¯æ•°é‡é™åˆ¶

        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        if session_id not in self.sessions:
            return []

        messages = self.sessions[session_id]["messages"]
        if limit:
            messages = messages[-limit:]

        return messages

    def get_user_sessions(self, user_id: str) -> List[str]:
        """
        è·å–ç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯

        Args:
            user_id: ç”¨æˆ·ID

        Returns:
            ä¼šè¯IDåˆ—è¡¨
        """
        return self.user_sessions.get(user_id, [])

    def delete_session(self, session_id: str) -> bool:
        """
        åˆ é™¤ä¼šè¯

        Args:
            session_id: ä¼šè¯ID

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        if session_id not in self.sessions:
            return False

        session = self.sessions[session_id]
        user_id = session["user_id"]

        del self.sessions[session_id]
        if user_id in self.user_sessions:
            self.user_sessions[user_id].remove(session_id)

        return True


class StreamingResponseHandler:
    """æµå¼å“åº”å¤„ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æµå¼å¤„ç†å™¨"""
        self.active_streams: Dict[str, Any] = {}

    def create_stream(self, stream_id: str) -> bool:
        """
        åˆ›å»ºæµ

        Args:
            stream_id: æµID

        Returns:
            æ˜¯å¦åˆ›å»ºæˆåŠŸ
        """
        if stream_id in self.active_streams:
            return False

        self.active_streams[stream_id] = {
            "id": stream_id,
            "created_at": int(time.time()),
            "chunks": [],
            "completed": False,
            "error": None
        }

        return True

    def add_chunk(self, stream_id: str, chunk: Dict[str, Any]) -> bool:
        """
        æ·»åŠ æµæ•°æ®å—

        Args:
            stream_id: æµID
            chunk: æ•°æ®å—

        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        if stream_id not in self.active_streams:
            return False

        self.active_streams[stream_id]["chunks"].append({
            "data": chunk,
            "timestamp": int(time.time())
        })

        return True

    def complete_stream(self, stream_id: str) -> bool:
        """
        å®Œæˆæµ

        Args:
            stream_id: æµID

        Returns:
            æ˜¯å¦å®ŒæˆæˆåŠŸ
        """
        if stream_id not in self.active_streams:
            return False

        self.active_streams[stream_id]["completed"] = True
        return True

    def set_stream_error(self, stream_id: str, error: str) -> bool:
        """
        è®¾ç½®æµé”™è¯¯

        Args:
            stream_id: æµID
            error: é”™è¯¯ä¿¡æ¯

        Returns:
            æ˜¯å¦è®¾ç½®æˆåŠŸ
        """
        if stream_id not in self.active_streams:
            return False

        self.active_streams[stream_id]["error"] = error
        self.active_streams[stream_id]["completed"] = True

        return True

    def get_stream_chunks(self, stream_id: str) -> List[Dict[str, Any]]:
        """
        è·å–æµæ•°æ®å—

        Args:
            stream_id: æµID

        Returns:
            æ•°æ®å—åˆ—è¡¨
        """
        if stream_id not in self.active_streams:
            return []

        return self.active_streams[stream_id]["chunks"]

    def stream_to_external(self, stream_id: str, callback=None):
        """
        å°†æµå¼è¾“å‡ºåˆ°å¤–éƒ¨

        Args:
            stream_id: æµID
            callback: å¤–éƒ¨å›è°ƒå‡½æ•°
        """
        if stream_id not in self.active_streams:
            return

        stream = self.active_streams[stream_id]

        for chunk_info in stream["chunks"]:
            chunk = chunk_info["data"]

            if callback:
                callback(chunk)
            else:
                # é»˜è®¤è¾“å‡ºåˆ°æ§åˆ¶å°
                print(f"[Stream {stream_id}] {chunk}")

    def get_stream_summary(self, stream_id: str) -> Dict[str, Any]:
        """
        è·å–æµæ‘˜è¦

        Args:
            stream_id: æµID

        Returns:
            æµæ‘˜è¦ä¿¡æ¯
        """
        if stream_id not in self.active_streams:
            return {}

        stream = self.active_streams[stream_id]
        chunks = stream["chunks"]

        return {
            "stream_id": stream_id,
            "created_at": stream["created_at"],
            "total_chunks": len(chunks),
            "completed": stream["completed"],
            "error": stream["error"],
            "duration": int(time.time()) - stream["created_at"] if chunks else 0
        }


class AgnoIntegratedExample:
    """agnoé›†æˆç¤ºä¾‹"""

    def __init__(self, api_key: str, base_url: str = None):
        """
        åˆå§‹åŒ–é›†æˆç¤ºä¾‹

        Args:
            api_key: APIå¯†é’¥
            base_url: APIç«¯ç‚¹
        """
        self.model_integration = OpenAIModelIntegration(
            api_key=api_key,
            base_url=base_url
        )
        self.session_manager = MultiUserSessionManager()
        self.stream_handler = StreamingResponseHandler()

    def basic_chat_example(self, user_id: str, message: str) -> Dict[str, Any]:
        """
        åŸºç¡€èŠå¤©ç¤ºä¾‹

        Args:
            user_id: ç”¨æˆ·ID
            message: ç”¨æˆ·æ¶ˆæ¯

        Returns:
            å“åº”ç»“æœ
        """
        # åˆ›å»ºæˆ–è·å–ä¼šè¯
        user_sessions = self.session_manager.get_user_sessions(user_id)
        if not user_sessions:
            session_id = self.session_manager.create_session(user_id)
        else:
            session_id = user_sessions[-1]  # ä½¿ç”¨æœ€æ–°çš„ä¼šè¯

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.session_manager.add_message(session_id, "user", message)

        # è·å–ä¼šè¯å†å²
        messages = self.session_manager.get_session_messages(session_id)

        # è½¬æ¢ä¸ºOpenAIæ ¼å¼
        openai_messages = [
            {"role": msg["role"], "content": msg["content"]}
            for msg in messages
        ]

        # è°ƒç”¨æ¨¡å‹
        response = self.model_integration.create_chat_completion(openai_messages)

        # æ·»åŠ åŠ©æ‰‹å“åº”
        if "content" in response:
            self.session_manager.add_message(
                session_id,
                "assistant",
                response["content"],
                usage=response.get("usage", {})
            )

        return {
            "session_id": session_id,
            "response": response
        }

    def streaming_chat_example(self, user_id: str, message: str):
        """
        æµå¼èŠå¤©ç¤ºä¾‹

        Args:
            user_id: ç”¨æˆ·ID
            message: ç”¨æˆ·æ¶ˆæ¯

        Yields:
            æµå¼å“åº”
        """
        # åˆ›å»ºä¼šè¯
        session_id = self.session_manager.create_session(user_id)
        stream_id = str(uuid4())

        # åˆ›å»ºæµ
        self.stream_handler.create_stream(stream_id)

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.session_manager.add_message(session_id, "user", message)

        # è·å–æ¶ˆæ¯å†å²
        messages = self.session_manager.get_session_messages(session_id)
        openai_messages = [
            {"role": msg["role"], "content": msg["content"]}
            for msg in messages
        ]

        # æµå¼è°ƒç”¨
        for chunk in self.model_integration.create_streaming_completion(openai_messages):
            # æ·»åŠ åˆ°æµå¤„ç†å™¨
            self.stream_handler.add_chunk(stream_id, chunk)

            # å®æ—¶è¾“å‡ºåˆ°å¤–éƒ¨
            yield {
                "session_id": session_id,
                "stream_id": stream_id,
                "chunk": chunk
            }

        # å®Œæˆæµ
        self.stream_handler.complete_stream(stream_id)

        # æ”¶é›†å®Œæ•´å“åº”
        full_response = ""
        for chunk_info in self.stream_handler.get_stream_chunks(stream_id):
            if "content" in chunk_info["data"]:
                full_response += chunk_info["data"]["content"]

        # æ·»åŠ å®Œæ•´å“åº”åˆ°ä¼šè¯
        self.session_manager.add_message(
            session_id,
            "assistant",
            full_response,
            stream_id=stream_id
        )

    def multi_user_parallel_example(self, users_data: List[Dict[str, str]]):
        """
        å¤šç”¨æˆ·å¹¶è¡Œå¤„ç†ç¤ºä¾‹

        Args:
            users_data: ç”¨æˆ·æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«user_idå’Œmessage

        Returns:
            å¹¶è¡Œå¤„ç†ç»“æœ
        """
        async def process_user(user_data):
            user_id = user_data["user_id"]
            message = user_data["message"]

            # åˆ›å»ºå¼‚æ­¥ä¼šè¯
            session_id = self.session_manager.create_session(user_id)

            # æ·»åŠ æ¶ˆæ¯
            self.session_manager.add_message(session_id, "user", message)

            # å¼‚æ­¥è°ƒç”¨æ¨¡å‹
            messages = self.session_manager.get_session_messages(session_id)
            openai_messages = [
                {"role": msg["role"], "content": msg["content"]}
                for msg in messages
            ]

            response = await self.model_integration.create_async_completion(openai_messages)

            # æ·»åŠ å“åº”
            if "content" in response:
                self.session_manager.add_message(
                    session_id,
                    "assistant",
                    response["content"],
                    usage=response.get("usage", {})
                )

            return {
                "user_id": user_id,
                "session_id": session_id,
                "response": response
            }

        # å¹¶å‘å¤„ç†æ‰€æœ‰ç”¨æˆ·
        async def process_all_users():
            tasks = [process_user(user_data) for user_data in users_data]
            return await asyncio.gather(*tasks, return_exceptions=True)

        return asyncio.run(process_all_users())


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºagnoåº“çš„æ ¸å¿ƒåŠŸèƒ½"""

    print("=" * 80)
    print("agnoåº“æ¶æ„åˆ†ææŠ¥å‘Š")
    print("=" * 80)

    # åˆ›å»ºåˆ†æå™¨
    analyzer = AgnoArchitectureAnalysis()

    # æ‰“å°åˆ†æç»“æœ
    print("\n1. OpenAIæ¨¡å‹é›†æˆåˆ†æ")
    print(json.dumps(analyzer.analyze_model_integration(), ensure_ascii=False, indent=2))

    print("\n2. å¤šç”¨æˆ·ä¼šè¯æ”¯æŒåˆ†æ")
    print(json.dumps(analyzer.analyze_multi_user_sessions(), ensure_ascii=False, indent=2))

    print("\n3. æµå¼è¾“å‡ºæœºåˆ¶åˆ†æ")
    print(json.dumps(analyzer.analyze_streaming_output(), ensure_ascii=False, indent=2))

    print("\n4. è¾“å…¥è¾“å‡ºæ ¼å¼åˆ†æ")
    print(json.dumps(analyzer.analyze_input_output_formats(), ensure_ascii=False, indent=2))

    print("\n" + "=" * 80)
    print("Pythonå®ç°æ¡ˆä¾‹æ¼”ç¤º")
    print("=" * 80)

    # æ³¨æ„ï¼šä»¥ä¸‹ä»£ç éœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥æ‰èƒ½è¿è¡Œ
    print("\næ³¨æ„ï¼šä»¥ä¸‹ç¤ºä¾‹éœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥")
    print("è¯·è®¾ç½®æ‚¨çš„APIå¯†é’¥åè¿è¡Œç›¸å…³ç¤ºä¾‹")

    # ç¤ºä¾‹ä»£ç ï¼ˆæ³¨é‡Šæ‰ï¼Œéœ€è¦çœŸå®APIå¯†é’¥ï¼‰
    example_code = '''
# ä½¿ç”¨ç¤ºä¾‹ï¼š
import os
from core.agno_analysis_report import AgnoIntegratedExample

# åˆå§‹åŒ–ï¼ˆè¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…APIå¯†é’¥ï¼‰
api_key = os.getenv("OPENAI_API_KEY", "your-api-key")
base_url = "https://api.openai.com/v1"  # æˆ–æ‚¨çš„è‡ªå®šä¹‰ç«¯ç‚¹

# åˆ›å»ºé›†æˆå®ä¾‹
agno_example = AgnoIntegratedExample(api_key=api_key, base_url=base_url)

# 1. åŸºç¡€èŠå¤©ç¤ºä¾‹
result = agno_example.basic_chat_example(
    user_id="user123",
    message="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹"
)
print("åŸºç¡€èŠå¤©ç»“æœ:", result)

# 2. æµå¼èŠå¤©ç¤ºä¾‹
print("\\næµå¼èŠå¤©å“åº”:")
for chunk in agno_example.streaming_chat_example(
    user_id="user123",
    message="è¯·è¯¦ç»†è§£é‡Šå¼‚æ­¥ç¼–ç¨‹çš„æ¦‚å¿µ"
):
    print(f"æµå¼æ•°æ®: {chunk}")

# 3. å¤šç”¨æˆ·å¹¶è¡Œå¤„ç†ç¤ºä¾‹
users_data = [
    {"user_id": "user1", "message": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
    {"user_id": "user2", "message": "è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ "},
    {"user_id": "user3", "message": "ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ"}
]

parallel_results = agno_example.multi_user_parallel_example(users_data)
print("\\nå¤šç”¨æˆ·å¹¶è¡Œç»“æœ:", parallel_results)

# 4. å•ç‹¬ä½¿ç”¨å„ä¸ªç»„ä»¶
from core.agno_analysis_report import OpenAIModelIntegration, MultiUserSessionManager

# æ¨¡å‹é›†æˆ
model = OpenAIModelIntegration(api_key=api_key)
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹"},
    {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"}
]
response = model.create_chat_completion(messages)
print("æ¨¡å‹å“åº”:", response)

# ä¼šè¯ç®¡ç†
session_manager = MultiUserSessionManager()
session_id = session_manager.create_session("user456")
session_manager.add_message(session_id, "user", "ä½ å¥½")
session_manager.add_message(session_id, "assistant", "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹")
messages = session_manager.get_session_messages(session_id)
print("ä¼šè¯æ¶ˆæ¯:", messages)
'''

    print("\nğŸ“„ ç¤ºä¾‹ä»£ç ï¼š")
    print(example_code)


if __name__ == "__main__":
    main()