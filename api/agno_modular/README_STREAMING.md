# Agno Modular æµå¼å¯¹è¯å’Œå¤šå‚å•†é€‚é…æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ agno_modular æ¨¡å—å®ç°å¤šå‚å•†æ¨¡å‹é€‚é…ã€æµå¼å¯¹è¯è¾“å‡ºå’Œ WebSocket é›†æˆåŠŸèƒ½ã€‚

## ğŸ¯ åŠŸèƒ½æ¦‚è§ˆ

### æ ¸å¿ƒç‰¹æ€§

- âœ… **å¤šå‚å•†æ¨¡å‹é€‚é…**: æ”¯æŒ OpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ç­‰å¤šç§ AI å‚å•†
- âœ… **æµå¼å¯¹è¯è¾“å‡º**: æ”¯æŒå¤šç§æµå¼è¾“å‡ºæ ¼å¼ï¼ˆSSEã€WebSocketã€ç”Ÿæˆå™¨ï¼‰
- âœ… **å¤šç§ç”Ÿæˆå™¨æ¨¡å¼**: æ ‡å‡†ã€ç¼“å†²ã€åˆ†å—ã€äº¤é”™ã€ä¼˜å…ˆçº§ç­‰æ¨¡å¼
- âœ… **WebSocket é›†æˆ**: å®æ—¶åŒå‘é€šä¿¡å’Œåé¦ˆæœºåˆ¶
- âœ… **Agent é›†æˆ**: ä¸ç°æœ‰çš„ Agent ç®¡ç†ç³»ç»Ÿæ— ç¼é›†æˆ
- âœ… **é”™è¯¯å¤„ç†**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé™çº§æœºåˆ¶
- âœ… **æ€§èƒ½ç›‘æ§**: å†…ç½®ç»Ÿè®¡å’Œæ€§èƒ½ç›‘æ§åŠŸèƒ½

## ğŸ“¦ ç»„ä»¶æ¶æ„

### 1. å¤šå‚å•†é€‚é…å™¨ (`multi_provider_adapter.py`)

```python
from multi_provider_adapter import (
    MultiProviderManager, ProviderConfig, ProviderType,
    register_openai_provider, register_anthropic_provider
)

# åˆ›å»ºç®¡ç†å™¨
manager = MultiProviderManager()

# æ³¨å†Œ OpenAI æä¾›å•†
await register_openai_provider(
    provider_id="openai_gpt35",
    api_key="your-api-key",
    model_name="gpt-3.5-turbo"
)

# æ³¨å†Œ Anthropic æä¾›å•†
await register_anthropic_provider(
    provider_id="anthropic_claude",
    api_key="your-api-key",
    model_name="claude-3-sonnet-20240229"
)

# ä½¿ç”¨æä¾›å•†è¿›è¡Œå¯¹è¯
adapter = manager.get_adapter("openai_gpt35")
async for event in adapter.stream_chat(messages):
    print(event)
```

### 2. æµå¼å¯¹è¯ (`streaming_chat.py`)

```python
from streaming_chat import (
    StreamingChatManager, StreamEvent, StreamFormat,
    create_streaming_chat_response
)

# åˆ›å»ºæµå¼èŠå¤©ç®¡ç†å™¨
manager = StreamingChatManager(provider_manager)

# ä½¿ç”¨æ ‡å‡†æµå¼è¾“å‡º
async for chunk in create_streaming_chat_response(
    messages=[{"role": "user", "content": "Hello"}],
    stream_format=StreamFormat.SSE
):
    print(chunk)
```

### 3. æµå¼ç”Ÿæˆå™¨ (`streaming_generator.py`)

```python
from streaming_generator import (
    StreamingGeneratorFactory, GeneratorMode, GeneratorConfig,
    streaming_context
)

# ä½¿ç”¨æ ‡å‡†ç”Ÿæˆå™¨
async with streaming_context(GeneratorMode.STANDARD) as generator:
    async for event in generator.generate(messages):
        print(event.data)

# ä½¿ç”¨ç¼“å†²ç”Ÿæˆå™¨
config = GeneratorConfig(
    mode=GeneratorMode.BUFFERED,
    buffer_size=1024,
    flush_interval=0.1
)

async with streaming_context(GeneratorMode.BUFFERED, config) as generator:
    async for event in generator.generate(messages):
        print(event.data)
```

### 4. WebSocket é›†æˆ (`websocket_integration.py`)

```python
from websocket_integration import (
    WebSocketManager, WebSocketMessage, WebSocketMessageType
)

# åˆ›å»º WebSocket ç®¡ç†å™¨
ws_manager = WebSocketManager(streaming_manager, provider_manager)

# æ·»åŠ è¿æ¥
connection = await ws_manager.add_connection(
    websocket=websocket,
    user_id="user123",
    session_id="session456"
)

# å‘é€æ¶ˆæ¯
message = WebSocketMessage(
    message_type=WebSocketMessageType.CHAT_REQUEST,
    data={"messages": [{"role": "user", "content": "Hello"}]}
)
await connection.send_message(message)
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€è®¾ç½®

```python
import asyncio
from multi_provider_adapter import get_multi_provider_manager
from streaming_chat import get_streaming_manager

async def setup():
    # è·å–å…¨å±€ç®¡ç†å™¨
    provider_manager = get_multi_provider_manager()
    streaming_manager = get_streaming_manager()

    # æ³¨å†Œæä¾›å•†
    await register_openai_provider(
        provider_id="openai_main",
        api_key="your-api-key",
        model_name="gpt-4"
    )

# è¿è¡Œè®¾ç½®
asyncio.run(setup())
```

### 2. ç®€å•æµå¼å¯¹è¯

```python
async def simple_streaming_chat():
    manager = get_streaming_manager()

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€ã€‚"}
    ]

    # åˆ›å»ºæµå¼å“åº”
    async for chunk in manager.create_session().stream_chat(
        messages=messages,
        stream_format=StreamFormat.SSE
    ):
        if chunk.startswith("data: "):
            data = json.loads(chunk[6:])
            if data.get("type") == "text-delta":
                print(data.get("content", ""), end="", flush=True)

# è¿è¡Œå¯¹è¯
asyncio.run(simple_streaming_chat())
```

### 3. WebSocket å®¢æˆ·ç«¯ç¤ºä¾‹

```javascript
// å‰ç«¯ WebSocket å®¢æˆ·ç«¯
const ws = new WebSocket('ws://localhost:8000/api/streaming/ws/user123');

ws.onopen = function(event) {
    console.log('WebSocketè¿æ¥å·²å»ºç«‹');

    // å‘é€èŠå¤©è¯·æ±‚
    ws.send(JSON.stringify({
        message_type: 'chat_request',
        data: {
            messages: [
                {role: 'user', content: 'ä½ å¥½ï¼'}
            ],
            generator_mode: 'standard'
        }
    }));
};

ws.onmessage = function(event) {
    const data = JSON.parse(event.data);

    if (data.message_type === 'stream_event') {
        const eventData = data.data;

        if (eventData.event_type === 'text-delta') {
            // å¤„ç†æ–‡æœ¬å¢é‡
            console.log(eventData.event_data.content);
        } else if (eventData.event_type === 'finish') {
            // å¤„ç†å®Œæˆäº‹ä»¶
            console.log('å¯¹è¯å®Œæˆ');
        }
    }
};
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. å¤šå‚å•†å¯¹æ¯”

```python
async def compare_providers():
    messages = [{"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "}]
    providers = ["openai_gpt35", "anthropic_claude", "agno_native"]

    results = {}

    for provider_id in providers:
        response = ""
        async for chunk in streaming_manager.create_session().stream_chat(
            messages=messages,
            provider_id=provider_id
        ):
            if "text-delta" in chunk:
                response += chunk
        results[provider_id] = response

    return results
```

### 2. è‡ªå®šä¹‰ç”Ÿæˆå™¨æ¨¡å¼

```python
from streaming_generator import StreamingGenerator

class CustomStreamingGenerator(StreamingGenerator):
    async def generate(self, messages, provider_id=None, **kwargs):
        # è‡ªå®šä¹‰ç”Ÿæˆé€»è¾‘
        adapter = self.provider_manager.get_adapter(provider_id)

        # æ·»åŠ è‡ªå®šä¹‰å¤„ç†
        async for provider_event in adapter.stream_chat(messages, **kwargs):
            # è‡ªå®šä¹‰äº‹ä»¶å¤„ç†
            event = self.custom_process_event(provider_event)
            yield event

    def custom_process_event(self, provider_event):
        # è‡ªå®šä¹‰äº‹ä»¶å¤„ç†é€»è¾‘
        return self._convert_to_stream_event(provider_event)

# ä½¿ç”¨è‡ªå®šä¹‰ç”Ÿæˆå™¨
generator = CustomStreamingGenerator(config, provider_manager)
```

### 3. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
async def robust_streaming_chat(messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            async for chunk in streaming_manager.create_session().stream_chat(
                messages=messages,
                provider_id="openai_gpt35"
            ):
                yield chunk
            break
        except Exception as e:
            if attempt == max_retries - 1:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œå‘é€é”™è¯¯äº‹ä»¶
                yield f'data: {json.dumps({"type": "error", "error": str(e)})}\n\n'
            else:
                logger.warning(f"Attempt {attempt + 1} failed, retrying...")
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

### 4. æ€§èƒ½ä¼˜åŒ–

```python
from streaming_generator import BufferedStreamingGenerator, GeneratorConfig

# ä½¿ç”¨ç¼“å†²æ¨¡å¼å‡å°‘ç½‘ç»œè¯·æ±‚
config = GeneratorConfig(
    mode=GeneratorMode.BUFFERED,
    buffer_size=512,  # ç¼“å†²512å­—èŠ‚
    flush_interval=0.2  # æ¯200msåˆ·æ–°ä¸€æ¬¡
)

async def optimized_streaming(messages):
    generator = BufferedStreamingGenerator(config, provider_manager)

    async for event in generator.generate(messages):
        # äº‹ä»¶å·²ç»è¿‡ç¼“å†²å¤„ç†
        yield event
```

## ğŸŒ API é›†æˆ

### FastAPI é›†æˆç¤ºä¾‹

```python
from fastapi import FastAPI
from agno_modular.streaming_api import router as streaming_router

app = FastAPI()

# æ³¨å†Œæµå¼ API è·¯ç”±
app.include_router(streaming_router)

# å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    from agno_modular.streaming_api import on_startup
    await on_startup()

# å…³é—­äº‹ä»¶
@app.on_event("shutdown")
async def shutdown_event():
    from agno_modular.streaming_api import on_shutdown
    await on_shutdown()
```

### API ç«¯ç‚¹ä½¿ç”¨

```bash
# æµå¼èŠå¤©
curl -X POST "http://localhost:8000/api/streaming/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼"}
    ],
    "stream_format": "sse",
    "provider_id": "openai_gpt35"
  }'

# æ³¨å†Œæä¾›å•†
curl -X POST "http://localhost:8000/api/streaming/providers/register" \
  -H "Content-Type: application/json" \
  -d '{
    "provider_id": "my_openai",
    "provider_type": "openai",
    "model_name": "gpt-4",
    "api_key": "your-api-key"
  }'

# WebSocket è¿æ¥
ws://localhost:8000/api/streaming/ws/user123?session_id=session456
```

## ğŸ“Š ç›‘æ§å’Œç»Ÿè®¡

### è·å–ç³»ç»ŸçŠ¶æ€

```python
async def get_system_stats():
    provider_manager = get_multi_provider_manager()
    streaming_manager = get_streaming_manager()
    websocket_manager = get_websocket_manager()

    stats = {
        "providers": await provider_manager.health_check_all(),
        "streaming": {
            "active_sessions": streaming_manager.get_active_sessions_count()
        },
        "websocket": websocket_manager.get_statistics()
    }

    return stats
```

### è‡ªå®šä¹‰ç›‘æ§

```python
class CustomMonitor:
    def __init__(self):
        self.request_count = 0
        self.error_count = 0
        self.response_times = []

    async def monitor_streaming_chat(self, messages, **kwargs):
        start_time = time.time()
        self.request_count += 1

        try:
            async for chunk in streaming_manager.create_session().stream_chat(
                messages, **kwargs
            ):
                yield chunk
        except Exception as e:
            self.error_count += 1
            raise
        finally:
            response_time = time.time() - start_time
            self.response_times.append(response_time)

    def get_stats(self):
        return {
            "total_requests": self.request_count,
            "error_rate": self.error_count / self.request_count,
            "avg_response_time": sum(self.response_times) / len(self.response_times)
        }
```

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest test_integration.py -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest test_integration.py::TestMultiProviderAdapter -v

# è¿è¡Œå¼‚æ­¥æµ‹è¯•
pytest test_integration.py::TestStreamingChat::test_streaming_session -v -s
```

### åŸºç¡€æµ‹è¯•

```python
async def run_basic_tests():
    from test_integration import run_basic_tests
    await run_basic_tests()
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: WebSocket è¿æ¥å¤±è´¥**
A: æ£€æŸ¥é˜²ç«å¢™è®¾ç½®ï¼Œç¡®ä¿ç«¯å£å¼€æ”¾ï¼ŒéªŒè¯ WebSocket URL æ­£ç¡®æ€§ã€‚

**Q: æä¾›å•†æ³¨å†Œå¤±è´¥**
A: éªŒè¯ API å¯†é’¥æ­£ç¡®æ€§ï¼Œæ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¡®è®¤æä¾›å•†æœåŠ¡å¯ç”¨ã€‚

**Q: æµå¼è¾“å‡ºä¸­æ–­**
A: æ£€æŸ¥ç½‘ç»œç¨³å®šæ€§ï¼ŒæŸ¥çœ‹é”™è¯¯æ—¥å¿—ï¼Œç¡®è®¤æä¾›å•†æ”¯æŒæµå¼è¾“å‡ºã€‚

**Q: æ€§èƒ½é—®é¢˜**
A: è€ƒè™‘ä½¿ç”¨ç¼“å†²æ¨¡å¼ï¼Œè°ƒæ•´ç¼“å†²åŒºå¤§å°ï¼Œç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µã€‚

### è°ƒè¯•æ¨¡å¼

```python
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)

# å¯ç”¨ç‰¹å®šæ¨¡å—çš„æ—¥å¿—
logger = logging.getLogger('agno_modular')
logger.setLevel(logging.DEBUG)
```

## ğŸ”® æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„æä¾›å•†

```python
from multi_provider_adapter import BaseProviderAdapter

class CustomProviderAdapter(BaseProviderAdapter):
    async def create_model(self):
        # å®ç°æ¨¡å‹åˆ›å»ºé€»è¾‘
        pass

    async def stream_chat(self, messages, tools=None, **kwargs):
        # å®ç°æµå¼èŠå¤©é€»è¾‘
        pass

    def supports_feature(self, feature):
        # å®ç°åŠŸèƒ½æ£€æŸ¥é€»è¾‘
        return True

# æ³¨å†Œè‡ªå®šä¹‰æä¾›å•†
def register_custom_provider(manager, provider_id, config):
    adapter = CustomProviderAdapter(config)
    manager._adapters[provider_id] = adapter
```

### æ·»åŠ æ–°çš„ç”Ÿæˆå™¨æ¨¡å¼

```python
from streaming_generator import StreamingGenerator

class CustomModeGenerator(StreamingGenerator):
    async def generate(self, messages, provider_id=None, **kwargs):
        # å®ç°è‡ªå®šä¹‰ç”Ÿæˆé€»è¾‘
        pass

# æ³¨å†Œåˆ°å·¥å‚
StreamingGeneratorFactory._generator_map[GeneratorMode.CUSTOM] = CustomModeGenerator
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [FastAPI WebSocket æ–‡æ¡£](https://fastapi.tiangolo.com/advanced/websockets/)
- [Server-Sent Events è§„èŒƒ](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)
- [Pydantic-AI æ–‡æ¡£](https://pydantic-ai.readthedocs.io/)
- [AsyncIO ç¼–ç¨‹æŒ‡å—](https://docs.python.org/3/library/asyncio.html)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥æ”¹è¿›è¿™ä¸ªæ¨¡å—ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚