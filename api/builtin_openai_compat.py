"""
OpenAI å…¼å®¹å±‚ for MLX-VLM å†…ç½®æ¨¡å‹

æä¾›æ ‡å‡†çš„ /v1/chat/completions æ¥å£ï¼Œå†…éƒ¨è°ƒç”¨ mlx_vlm åº“
"""
import logging
import asyncio
import gc
import time
import json
import uuid
from config import singleton
from utils import preprocess_image
from enum import IntEnum
from typing import Optional, List, Dict, Any, Literal
from pydantic import BaseModel, Field
from fastapi.responses import StreamingResponse
import mlx.core as mx
from mlx_vlm.generate import stream_generate, generate
from mlx_vlm.prompt_utils import apply_chat_template
from mlx_vlm.utils import load

# ğŸ”’ å¯¼å…¥ Metal GPU äº’æ–¥é”
# from multivector_mgr import acquire_metal_lock_async, release_metal_lock_async

logger = logging.getLogger(__name__)

# ==================== ä¼˜å…ˆçº§å®šä¹‰ ====================

class RequestPriority(IntEnum):
    """è¯·æ±‚ä¼˜å…ˆçº§"""
    HIGH = 1    # ä¼šè¯ç•Œé¢è¯·æ±‚ï¼ˆç”¨æˆ·ä¸»åŠ¨å‘èµ·ï¼‰
    LOW = 10    # æ‰¹é‡ä»»åŠ¡è¯·æ±‚ï¼ˆåå°è‡ªåŠ¨ï¼‰

# ==================== OpenAI API æ¨¡å‹å®šä¹‰ ====================

class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: Literal["system", "user", "assistant"] = Field(...)
    content: str | List[Dict[str, Any]] = Field(...)  # æ”¯æŒæ–‡æœ¬æˆ–å¤šæ¨¡æ€å†…å®¹

class OpenAIChatCompletionRequest(BaseModel):
    """OpenAI /v1/chat/completions è¯·æ±‚æ ¼å¼"""
    model: str = Field(..., description="æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆHuggingFace è·¯å¾„ï¼‰")
    messages: List[ChatMessage] = Field(..., description="èŠå¤©æ¶ˆæ¯åˆ—è¡¨")
    temperature: float = Field(default=0.7, ge=0, le=2)
    max_tokens: Optional[int] = Field(default=512, ge=1)
    top_p: float = Field(default=1.0, ge=0, le=1)
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼è¿”å›")
    response_format: Optional[Dict[str, Any]] = Field(default=None, description="å“åº”æ ¼å¼é…ç½®ï¼ˆæ¥å—ä»»ä½•æ ¼å¼ï¼Œä¸»è¦ç”¨äºå…¼å®¹æ€§ï¼‰")
    # æ‰©å±•å­—æ®µï¼ˆé OpenAI æ ‡å‡†ï¼‰
    images: Optional[List[str]] = Field(default=None, description="å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰")

class ChatCompletionChoice(BaseModel):
    """å•ä¸ªå®Œæˆé€‰é¡¹"""
    index: int
    message: ChatMessage
    finish_reason: Literal["stop", "length"] = "stop"

class ChatCompletionUsage(BaseModel):
    """Token ä½¿ç”¨ç»Ÿè®¡"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class OpenAIChatCompletionResponse(BaseModel):
    """OpenAI /v1/chat/completions å“åº”æ ¼å¼"""
    id: str
    object: Literal["chat.completion"] = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: ChatCompletionUsage

class ChatCompletionChunk(BaseModel):
    """æµå¼å“åº”çš„å•ä¸ªå—"""
    id: str
    object: Literal["chat.completion.chunk"] = "chat.completion.chunk"
    created: int
    model: str
    choices: List[Dict[str, Any]]  # delta æ ¼å¼

# ==================== æ¨¡å‹ç®¡ç†ï¼ˆå¸¦ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼‰ ====================

@singleton
class MLXVLMModelManager:
    """
    MLX-VLM æ¨¡å‹ç®¡ç†å™¨
    
    ç‰¹æ€§ï¼š
    - æŒ‰éœ€åŠ è½½ï¼šé¦–æ¬¡è¯·æ±‚æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
    - å¹¶å‘ä¿æŠ¤ï¼šä½¿ç”¨ asyncio.Lock é˜²æ­¢å¹¶å‘åŠ è½½
    - ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼šé«˜ä¼˜å…ˆçº§è¯·æ±‚ï¼ˆä¼šè¯ï¼‰ä¼˜å…ˆå¤„ç†
    - è‡ªåŠ¨é˜Ÿåˆ—å¤„ç†ï¼šåå°ä»»åŠ¡å¾ªç¯å¤„ç†è¯·æ±‚
    """
    
    def __init__(self):
        self._model_cache: Dict[str, Any] = {}
        self._lock = asyncio.Lock()
        self._request_queue = asyncio.PriorityQueue()
        self._processing_task: Optional[asyncio.Task] = None
        self._is_processing = False
        self._request_counter = 0  # ç”¨äºæ‰“ç ´ä¼˜å…ˆçº§å¹³å±€
    
    async def ensure_loaded(self, model_path: str):
        """
        ç¡®ä¿æ¨¡å‹å·²åŠ è½½ï¼ˆæŒ‰éœ€åŠ è½½ + å¹¶å‘ä¿æŠ¤ï¼‰
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
        """
        async with self._lock:
            # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½
            if self._model_cache.get("model_path") == model_path:
                logger.debug(f"Model already loaded: {model_path}")
                return
            
            # æ¸…ç†æ—§æ¨¡å‹
            if self._model_cache:
                # logger.info("Clearing old model from cache...")
                await self._unload_model_internal()
            
            # åŠ è½½æ–°æ¨¡å‹
            # logger.info(f"Loading model: {model_path}")
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„ load æ“ä½œ
            loop = asyncio.get_event_loop()
            model, processor = await loop.run_in_executor(
                None,
                lambda: load(model_path, trust_remote_code=True)
            )
            config = model.config
            
            self._model_cache = {
                "model_path": model_path,
                "model": model,
                "processor": processor,
                "config": config
            }
            
            # logger.info(f"Model loaded successfully: {model_path}")
            
            # å¯åŠ¨é˜Ÿåˆ—å¤„ç†å™¨ï¼ˆå¦‚æœè¿˜æ²¡å¯åŠ¨ï¼‰
            if not self._is_processing:
                self._processing_task = asyncio.create_task(self._process_queue())
    
    async def enqueue_request(
        self,
        request: "OpenAIChatCompletionRequest",
        model_path: str,
        priority: RequestPriority = RequestPriority.LOW
    ):
        """
        å°†è¯·æ±‚åŠ å…¥ä¼˜å…ˆçº§é˜Ÿåˆ—
        
        Args:
            request: OpenAI æ ¼å¼çš„è¯·æ±‚
            model_path: æ¨¡å‹è·¯å¾„
            priority: è¯·æ±‚ä¼˜å…ˆçº§
        
        Returns:
            å“åº”ç»“æœï¼ˆé˜»å¡ç­‰å¾…ï¼‰
        """
        # # éªŒè¯ response_format å‚æ•°ï¼ˆå®½å®¹å¤„ç†ï¼Œä»…è®°å½•ï¼‰
        # if request.response_format:
        #     response_type = request.response_format.get("type", "text")
            # logger.info(f"Response format requested: {response_type}")
            # æ³¨æ„ï¼šæœ¬åœ°æ¨¡å‹æ— æ³•å¼ºåˆ¶æ‰§è¡Œ JSON schemaï¼Œä¾èµ–ç³»ç»Ÿæç¤ºè¯æŒ‡å¯¼
        
        # ç¡®ä¿é˜Ÿåˆ—å¤„ç†å™¨å·²å¯åŠ¨
        if not self._is_processing:
            # logger.info("Starting queue processor (first request)")
            self._processing_task = asyncio.create_task(self._process_queue())
        
        future = asyncio.Future()
        # ä½¿ç”¨è®¡æ•°å™¨ä½œä¸ºç¬¬äºŒä¸ªæ’åºé”®ï¼Œé¿å…æ¯”è¾ƒ Pydantic å¯¹è±¡
        # æ ¼å¼: (priority, counter, request, model_path, future)
        self._request_counter += 1
        await self._request_queue.put((priority, self._request_counter, request, model_path, future))
        logger.debug(f"Request enqueued with priority {priority.name}, queue size: {self._request_queue.qsize()}")
        
        # ç­‰å¾…ç»“æœ
        return await future
    
    async def _process_queue(self):
        """
        é˜Ÿåˆ—å¤„ç†å™¨ï¼ˆåå°ä»»åŠ¡ï¼‰
        
        å¾ªç¯å¤„ç†é˜Ÿåˆ—ä¸­çš„è¯·æ±‚ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºã€‚
        ç©ºé—² 60 ç§’åè‡ªåŠ¨é€€å‡ºã€‚
        """
        self._is_processing = True
        # logger.info("Queue processor started")
        
        try:
            while True:
                try:
                    # å¸¦è¶…æ—¶çš„é˜Ÿåˆ—è·å–ï¼ˆé¿å…æ°¸ä¹…é˜»å¡ï¼‰
                    priority, counter, request, model_path, future = await asyncio.wait_for(
                        self._request_queue.get(),
                        timeout=60.0
                    )
                    
                    # logger.info(f"Processing request #{counter} with priority {priority.name} (queue size: {self._request_queue.qsize()})")
                    
                    try:
                        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
                        await self.ensure_loaded(model_path)
                        
                        # æ‰§è¡Œæ¨ç†
                        result = await self._generate_completion_internal(request, model_path)
                        future.set_result(result)
                        
                    except Exception as e:
                        logger.error(f"Request processing failed: {e}", exc_info=True)
                        future.set_exception(e)
                    
                except asyncio.TimeoutError:
                    # é˜Ÿåˆ—ç©ºé—²è¶…æ—¶ï¼Œé€€å‡ºå¤„ç†å™¨
                    # logger.info("Queue idle for 60s, stopping processor")
                    break
                    
        finally:
            self._is_processing = False
            # logger.info("Queue processor stopped")
    
    async def _generate_completion_internal(
        self,
        request: "OpenAIChatCompletionRequest",
        model_path: str
    ):
        """
        å†…éƒ¨æ¨ç†æ–¹æ³•ï¼ˆä¸ç»è¿‡é˜Ÿåˆ—ï¼‰
        
        è¿™ä¸ªæ–¹æ³•ä¼šè¢«é˜Ÿåˆ—å¤„ç†å™¨è°ƒç”¨ï¼Œæ‰§è¡Œå®é™…çš„æ¨¡å‹æ¨ç†ã€‚
        """
        # è·å–å·²åŠ è½½çš„æ¨¡å‹
        model = self._model_cache["model"]
        processor = self._model_cache["processor"]
        config = self._model_cache["config"]
        
        # # ğŸ” è°ƒè¯•ï¼šæŸ¥çœ‹æ”¶åˆ°çš„åŸå§‹ messages æ ¼å¼
        # logger.info(f"ğŸ“¨ æ”¶åˆ° {len(request.messages)} æ¡æ¶ˆæ¯")
        # for i, msg in enumerate(request.messages):
        #     logger.info(f"  æ¶ˆæ¯[{i}] role={msg.role}, contentç±»å‹={type(msg.content).__name__}")
        #     if isinstance(msg.content, list):
        #         logger.info(f"    contenté•¿åº¦={len(msg.content)}")
        #         # æ‰“å°æ¯ä¸ªå…ƒç´ çš„ç±»å‹å’Œå†…å®¹æ‘˜è¦
        #         for j, item in enumerate(msg.content):
        #             if isinstance(item, dict):
        #                 item_type = item.get('type', 'unknown')
        #                 if item_type == 'text':
        #                     text_preview = item.get('text', '')[:50]
        #                     logger.info(f"      [{j}] type=text, preview={text_preview}")
        #                 elif item_type == 'image_url':
        #                     url_obj = item.get('image_url', {})
        #                     if isinstance(url_obj, dict):
        #                         url = url_obj.get('url', '')
        #                         # åªæ˜¾ç¤ºå‰50å­—ç¬¦ï¼Œé¿å…æ‰“å°æ•´ä¸ªbase64
        #                         url_preview = url[:50] + ('...' if len(url) > 50 else '')
        #                         logger.info(f"      [{j}] type=image_url, url={url_preview}")
        #                     else:
        #                         logger.info(f"      [{j}] type=image_url, url={url_obj}")
        #                 else:
        #                     logger.info(f"      [{j}] type={item_type}, keys={list(item.keys())}")
        #             else:
        #                 logger.info(f"      [{j}] édictç±»å‹: {type(item).__name__}")
            # elif isinstance(msg.content, str):
                # å¯¹äºç»“æ„åŒ–JSONæ•°æ®ï¼ˆå¦‚æ ‡ç­¾ï¼‰ï¼Œæ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼›å…¶ä»–æƒ…å†µé™åˆ¶é•¿åº¦
                # if msg.content.strip().startswith('{') or msg.content.strip().startswith('['):
                    # JSONæ ¼å¼ï¼Œæ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼ˆæœ€å¤š1000å­—ç¬¦ï¼Œæ ‡ç­¾æ•°æ®é€šå¸¸å¾ˆå°ï¼‰
                    # content_display = msg.content if len(msg.content) <= 1000 else msg.content[:1000] + '...'
                    # logger.info(f"    content(JSONå®Œæ•´): {content_display}")
                # else:
                    # éJSONï¼Œåªæ˜¾ç¤ºå‰50å­—ç¬¦
                    # logger.info(f"    contentå‰50å­—ç¬¦: {msg.content[:50]}")
        
        # æå–å›¾ç‰‡å’Œæ–‡æœ¬ï¼ˆè¿”å›å­—å…¸åˆ—è¡¨ï¼Œç”¨äº apply_chat_templateï¼‰
        message_dicts, image_urls = _extract_images_from_messages(request.messages)
        
        # logger.info(f"Extracted {len(message_dicts)} messages and {len(image_urls)} images")
        # logger.info("ğŸ“ Message dicts for apply_chat_template:")
        # for i, msg_dict in enumerate(message_dicts):
            # å¯¹äºJSONæ ¼å¼æ˜¾ç¤ºå®Œæ•´å†…å®¹
            # content = msg_dict['content']
            # if content.strip().startswith('{') or content.strip().startswith('['):
                # content_display = content if len(content) <= 1000 else content[:1000] + '...'
                # logger.info(f"  [{i}] role={msg_dict['role']}, content={content_display}")
            # else:
            #     content_preview = content[:50] if len(content) > 50 else content
                # logger.info(f"  [{i}] role={msg_dict['role']}, content={content_preview}...")
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿ï¼ˆç›´æ¥ä½¿ç”¨å­—å…¸åˆ—è¡¨ï¼‰
        formatted_prompt = apply_chat_template(
            processor,
            config,
            message_dicts,
            num_images=len(image_urls),
            num_audios=0
        )
        
        # logger.info(f"ğŸ”¤ Formatted prompt preview (first 200 chars): {formatted_prompt[:200]}")
        # logger.info(f"ğŸ”¤ Formatted prompt contains '<|vision_start|>': {'<|vision_start|>' in formatted_prompt}")
        # logger.info(f"ğŸ”¤ Formatted prompt contains '<|image_pad|>': {'<|image_pad|>' in formatted_prompt}")
        
        # æ‰§è¡Œæ¨ç†
        if request.stream:
            # æµå¼å“åº”
            return await self._generate_streaming_response(
                request, model, processor, formatted_prompt, image_urls
            )
        else:
            # éæµå¼å“åº”
            return await self._generate_non_streaming_response(
                request, model, processor, formatted_prompt, image_urls
            )
    
    async def _generate_streaming_response(
        self,
        request: "OpenAIChatCompletionRequest",
        model, processor, prompt, images
    ):
        """ç”Ÿæˆæµå¼å“åº”"""
        
        response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
        created_at = int(time.time())
        
        async def stream_generator():
            # ğŸ”’ è·å– Metal GPU é”
            # await acquire_metal_lock_async("MLX-VLM streaming")
            try:
                # logger.info("Starting streaming generation")
                token_iterator = stream_generate(
                    model=model,
                    processor=processor,
                    prompt=prompt,
                    image=images if images else None,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens,
                    top_p=request.top_p
                )
                
                chunk_count = 0
                for chunk in token_iterator:
                    chunk_count += 1
                    if chunk is None or not hasattr(chunk, "text"):
                        continue
                    
                    if chunk.text:
                        chunk_data = ChatCompletionChunk(
                            id=response_id,
                            created=created_at,
                            model=request.model,
                            choices=[{
                                "index": 0,
                                "delta": {"content": chunk.text},
                                "finish_reason": None
                            }]
                        )
                        yield f"data: {chunk_data.model_dump_json()}\n\n"
                        await asyncio.sleep(0.01)
                
                # logger.info(f"Streaming completed: {chunk_count} chunks")
                
                # å‘é€ç»“æŸæ ‡è®°
                final_chunk = ChatCompletionChunk(
                    id=response_id,
                    created=created_at,
                    model=request.model,
                    choices=[{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                )
                yield f"data: {final_chunk.model_dump_json()}\n\n"
                yield "data: [DONE]\n\n"
                
            except Exception as e:
                logger.error(f"Streaming error: {e}", exc_info=True)
                error_chunk = {"error": {"message": str(e), "type": "internal_error"}}
                yield f"data: {json.dumps(error_chunk)}\n\n"
            # finally:
                # ğŸ”“ é‡Šæ”¾ Metal GPU é”
                # await release_metal_lock_async("MLX-VLM streaming")
        
        return StreamingResponse(
            stream_generator(),
            media_type="text/event-stream"
        )
    
    async def _generate_non_streaming_response(
        self,
        request: "OpenAIChatCompletionRequest",
        model, processor, prompt, images
    ):
        """ç”Ÿæˆéæµå¼å“åº”"""
        
        response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
        created_at = int(time.time())
        
        # logger.info("Starting non-streaming generation")
        
        # ğŸ”’ è·å– Metal GPU é”
        # await acquire_metal_lock_async("MLX-VLM non-streaming")
        try:
            # æ„é€ å‚æ•°å­—å…¸,åªåœ¨æœ‰å›¾ç‰‡æ—¶ä¼ é€’ image å‚æ•°
            generate_kwargs = {
                "model": model,
                "processor": processor,
                "prompt": prompt,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens or 512,
                "top_p": request.top_p,
                "verbose": False
            }
            if images:
                generate_kwargs["image"] = images
            
            # ä½¿ç”¨ generate å‡½æ•°è¿›è¡Œéæµå¼æ¨ç†
            result = generate(**generate_kwargs)
            
            result_text = result.text if hasattr(result, 'text') else str(result)
            # logger.info(f"Generation completed: {len(result_text)} chars")
        finally:
            # ğŸ”“ é‡Šæ”¾ Metal GPU é”
            # await release_metal_lock_async("MLX-VLM non-streaming")
            pass
        
        # æ„é€  OpenAI æ ¼å¼å“åº”
        response = OpenAIChatCompletionResponse(
            id=response_id,
            created=created_at,
            model=request.model,
            choices=[ChatCompletionChoice(
                index=0,
                message=ChatMessage(role="assistant", content=result_text),
                finish_reason="stop"
            )],
            usage=ChatCompletionUsage(
                prompt_tokens=getattr(result, 'prompt_tokens', 0),
                completion_tokens=getattr(result, 'generation_tokens', len(result_text) // 4),
                total_tokens=getattr(result, 'total_tokens', len(result_text) // 4)
            )
        )
        
        return response
    
    async def _unload_model_internal(self):
        """å†…éƒ¨å¸è½½æ–¹æ³•"""
        if not self._model_cache:
            return
        
        # logger.info(f"Unloading model: {self._model_cache.get('model_path')}")
        self._model_cache = {}
        gc.collect()
        mx.clear_cache()
        # logger.info("Model unloaded and cache cleared")
    
    async def unload_model(self):
        """å…¬å…±å¸è½½æ–¹æ³•ï¼ˆå¸¦é”ä¿æŠ¤ï¼‰"""
        async with self._lock:
            await self._unload_model_internal()
    
    async def check_and_unload_if_unused(self, engine):
        """
        æ™ºèƒ½å¸è½½é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•èƒ½åŠ›ä»åœ¨ä½¿ç”¨ MLX-VLM æ¨¡å‹
        
        å¦‚æœ 4 ä¸ªèƒ½åŠ›ï¼ˆVISION, TEXT, STRUCTURED_OUTPUT, TOOL_USEï¼‰éƒ½å·²åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å‹ï¼Œ
        åˆ™è‡ªåŠ¨å¸è½½ MLX-VLM æ¨¡å‹ä»¥é‡Šæ”¾å†…å­˜ã€‚
        
        Args:
            engine: SQLAlchemy engine ç”¨äºæŸ¥è¯¢æ•°æ®åº“
        """
        from sqlmodel import Session, select
        from db_mgr import CapabilityAssignment, ModelConfiguration
        
        # MLX-VLM çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆä¸æ•°æ®åº“ä¸­çš„ model_identifier ä¸€è‡´ï¼‰
        MLX_VLM_MODEL_IDENTIFIER = "mlx-community/Qwen3-VL-4B-Instruct-3bit"
        
        with Session(engine) as session:
            # æŸ¥è¯¢æ‰€æœ‰èƒ½åŠ›åˆ†é…
            assignments = session.exec(select(CapabilityAssignment)).all()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•èƒ½åŠ›ä»åœ¨ä½¿ç”¨ MLX-VLM
            for assignment in assignments:
                model_config = session.exec(
                    select(ModelConfiguration).where(
                        ModelConfiguration.id == assignment.model_configuration_id
                    )
                ).first()
                
                if model_config and model_config.model_identifier == MLX_VLM_MODEL_IDENTIFIER:
                    # logger.info(
                    #     f"Capability {assignment.capability_value} still using MLX-VLM, "
                    #     f"skipping unload"
                    # )
                    return False
            
            # æ‰€æœ‰èƒ½åŠ›éƒ½å·²åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å‹ï¼Œå¸è½½ MLX-VLM
            # logger.info("All capabilities switched away from MLX-VLM, unloading model...")
            await self.unload_model()
            # logger.info("MLX-VLM model unloaded successfully")
            return True
    
    def is_model_loaded(self, model_path: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return self._model_cache.get("model_path") == model_path
    
    def get_queue_size(self) -> int:
        """è·å–å½“å‰é˜Ÿåˆ—å¤§å°"""
        return self._request_queue.qsize()


# ==================== è¯·æ±‚è½¬æ¢é€»è¾‘ ====================

def _extract_images_from_messages(messages: List[ChatMessage]) -> tuple[List[Dict[str, Any]], List[str]]:
    """
    ä» OpenAI æ¶ˆæ¯æ ¼å¼ä¸­æå–å›¾ç‰‡ URLï¼Œä½†ä¿æŒæ¶ˆæ¯ç»“æ„ç”¨äº apply_chat_template
    
    æ”¯æŒä¸¤ç§æ ¼å¼:
    1. content ä¸º list: [{"type": "text", "text": "..."}, {"type": "image_url", "image_url": {"url": "..."}}]
    2. content ä¸º str: çº¯æ–‡æœ¬
    
    é‡è¦ï¼šä¼šåˆå¹¶å¤šä¸ªè¿ç»­çš„ system æ¶ˆæ¯ä¸ºä¸€æ¡ï¼Œä»¥ç¡®ä¿ apply_chat_template æ­£ç¡®æ’å…¥å›¾ç‰‡å ä½ç¬¦
    
    Returns:
        (æ¶ˆæ¯å­—å…¸åˆ—è¡¨ï¼ˆç”¨äº apply_chat_templateï¼‰, å›¾ç‰‡URLåˆ—è¡¨ï¼ˆç”¨äºä¼ é€’ç»™ generate/stream_generateï¼‰)
    """
    message_dicts = []
    image_urls = []
    system_messages = []  # ä¸´æ—¶å­˜å‚¨è¿ç»­çš„ system æ¶ˆæ¯
    
    def flush_system_messages():
        """åˆå¹¶å¹¶æ·»åŠ ç´¯ç§¯çš„ system æ¶ˆæ¯"""
        if system_messages:
            combined_system = "\n\n".join(system_messages)
            message_dicts.append({
                "role": "system",
                "content": combined_system
            })
            system_messages.clear()
    
    for msg in messages:
        if isinstance(msg.content, str):
            # çº¯æ–‡æœ¬æ¶ˆæ¯
            if msg.role == "system":
                system_messages.append(msg.content)
            else:
                # å…ˆåˆ·æ–°ç´¯ç§¯çš„ system æ¶ˆæ¯
                flush_system_messages()
                message_dicts.append({
                    "role": msg.role,
                    "content": msg.content
                })
        elif isinstance(msg.content, list):
            # å¤šæ¨¡æ€æ¶ˆæ¯ - æå–æ–‡æœ¬å’Œå›¾ç‰‡
            text_parts = []
            for part in msg.content:
                if isinstance(part, dict):
                    if part.get("type") == "text":
                        text_parts.append(part["text"])
                    elif part.get("type") == "image_url":
                        # æå–å›¾ç‰‡ URL å¹¶é¢„å¤„ç†
                        url = part.get("image_url", {}).get("url") or part.get("image_url")
                        if url:
                            # é¢„å¤„ç†å›¾ç‰‡ï¼ˆå‹ç¼©å¤§å›¾ç‰‡ï¼‰
                            processed_url = preprocess_image(url, max_size=1920, quality=85)
                            image_urls.append(processed_url)
            
            # åˆå¹¶æ–‡æœ¬éƒ¨åˆ†å¹¶ä¿å­˜ä¸ºå­—å…¸
            combined_text = " ".join(text_parts)
            if combined_text:
                if msg.role == "system":
                    system_messages.append(combined_text)
                else:
                    # å…ˆåˆ·æ–°ç´¯ç§¯çš„ system æ¶ˆæ¯
                    flush_system_messages()
                    message_dicts.append({
                        "role": msg.role,
                        "content": combined_text
                    })
    
    # åˆ·æ–°æœ€åçš„ system æ¶ˆæ¯
    flush_system_messages()
    
    return message_dicts, image_urls


# å…¨å±€å•ä¾‹
_model_manager = MLXVLMModelManager()
def get_vlm_manager() -> MLXVLMModelManager:
    """è·å–å…¨å±€ VLM æ¨¡å‹ç®¡ç†å™¨å•ä¾‹"""
    return _model_manager

# ==================== æµ‹è¯•ä»£ç  ====================
if __name__ == "__main__":
    import asyncio
    
    async def test_lazy_loading():
        """æµ‹è¯•1: æ‡’åŠ è½½"""
        print("\n" + "="*60)
        print("æµ‹è¯• 1: æ‡’åŠ è½½")
        print("="*60)
        
        manager = get_vlm_manager()
        model_path = "/Users/dio/Library/Application Support/knowledge-focus.huozhong.in/builtin_models/models--mlx-community--Qwen3-VL-4B-Instruct-3bit/snapshots/629882a0df4a41662d0b6d6aa7aedf56032501fd"
        
        # é¦–æ¬¡åŠ è½½ä¼šè§¦å‘æ¨¡å‹åŠ è½½
        await manager.ensure_loaded(model_path)
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {manager.is_model_loaded(model_path)}")
        print(f"   é˜Ÿåˆ—å¤§å°: {manager.get_queue_size()}")
    
    async def test_simple_chat():
        """æµ‹è¯•2: ç®€å•èŠå¤©"""
        print("\n" + "="*60)
        print("æµ‹è¯• 2: ç®€å•èŠå¤©ï¼ˆéæµå¼ï¼‰")
        print("="*60)
        
        manager = get_vlm_manager()
        model_path = "/Users/dio/Library/Application Support/knowledge-focus.huozhong.in/builtin_models/models--mlx-community--Qwen3-VL-4B-Instruct-3bit/snapshots/629882a0df4a41662d0b6d6aa7aedf56032501fd"
        
        # æ„é€ è¯·æ±‚
        request = OpenAIChatCompletionRequest(
            model="qwen3-vl-4b",
            messages=[
                ChatMessage(role="user", content="ç”¨ä¸€å¥è¯ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€")
            ],
            max_tokens=100,
            temperature=0.7,
            stream=False
        )
        
        print("ğŸš€ å‘é€è¯·æ±‚...")
        response = await manager.enqueue_request(
            request=request,
            model_path=model_path,
            priority=RequestPriority.HIGH
        )
        
        print("âœ… æ”¶åˆ°å“åº”:")
        print(f"   ID: {response.id}")
        print(f"   å†…å®¹: {response.choices[0].message.content}")
        print(f"   Tokens: {response.usage.total_tokens}")
    
    async def test_streaming_chat():
        """æµ‹è¯•3: æµå¼èŠå¤©"""
        print("\n" + "="*60)
        print("æµ‹è¯• 3: æµå¼èŠå¤©")
        print("="*60)
        
        manager = get_vlm_manager()
        model_path = "/Users/dio/Library/Application Support/knowledge-focus.huozhong.in/builtin_models/models--mlx-community--Qwen3-VL-4B-Instruct-3bit/snapshots/629882a0df4a41662d0b6d6aa7aedf56032501fd"
        
        # æ„é€ æµå¼è¯·æ±‚
        request = OpenAIChatCompletionRequest(
            model="qwen3-vl-4b",
            messages=[
                ChatMessage(role="user", content="åˆ—ä¸¾3ä¸ªPythonçš„ä¼˜ç‚¹")
            ],
            max_tokens=150,
            temperature=0.7,
            stream=True
        )
        
        print("ğŸš€ å‘é€æµå¼è¯·æ±‚...")
        response = await manager.enqueue_request(
            request=request,
            model_path=model_path,
            priority=RequestPriority.HIGH
        )
        
        # StreamingResponse éœ€è¦é€šè¿‡å¼‚æ­¥è¿­ä»£å™¨è¯»å–
        print("âœ… æ”¶åˆ°æµå¼å“åº”:")
        print("   ", end="", flush=True)
        
        # æ³¨æ„: StreamingResponse çš„ body_iterator æ˜¯å¼‚æ­¥ç”Ÿæˆå™¨
        async for chunk in response.body_iterator:
            chunk_str = chunk.decode('utf-8') if isinstance(chunk, bytes) else chunk
            if chunk_str.startswith('data: '):
                data_str = chunk_str[6:].strip()
                if data_str and data_str != '[DONE]':
                    try:
                        import json
                        data = json.loads(data_str)
                        if 'choices' in data and len(data['choices']) > 0:
                            delta = data['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                print(content, end="", flush=True)
                    except json.JSONDecodeError:
                        pass
        print("\n")
    
    async def test_priority_queue():
        """æµ‹è¯•4: ä¼˜å…ˆçº§é˜Ÿåˆ—"""
        print("\n" + "="*60)
        print("æµ‹è¯• 4: ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆå¹¶å‘è¯·æ±‚ï¼‰")
        print("="*60)
        
        manager = get_vlm_manager()
        model_path = "/Users/dio/Library/Application Support/knowledge-focus.huozhong.in/builtin_models/models--mlx-community--Qwen3-VL-4B-Instruct-3bit/snapshots/629882a0df4a41662d0b6d6aa7aedf56032501fd"
        
        # æ¨¡æ‹Ÿä½ä¼˜å…ˆçº§è¯·æ±‚ï¼ˆæ‰¹é‡ä»»åŠ¡ï¼‰
        low_request = OpenAIChatCompletionRequest(
            model="qwen3-vl-4b",
            messages=[ChatMessage(role="user", content="è¯´'ä½ä¼˜å…ˆçº§'")],
            max_tokens=20,
            stream=False
        )
        
        # æ¨¡æ‹Ÿé«˜ä¼˜å…ˆçº§è¯·æ±‚ï¼ˆç”¨æˆ·èŠå¤©ï¼‰
        high_request = OpenAIChatCompletionRequest(
            model="qwen3-vl-4b",
            messages=[ChatMessage(role="user", content="è¯´'é«˜ä¼˜å…ˆçº§'")],
            max_tokens=20,
            stream=False
        )
        
        print("ğŸš€ åŒæ—¶å‘é€ä½ä¼˜å…ˆçº§å’Œé«˜ä¼˜å…ˆçº§è¯·æ±‚...")
        
        # å¹¶å‘å‘é€
        low_task = asyncio.create_task(
            manager.enqueue_request(low_request, model_path, RequestPriority.LOW)
        )
        await asyncio.sleep(0.1)  # ç¨å¾®å»¶è¿Ÿï¼Œè®©ä½ä¼˜å…ˆçº§å…ˆå…¥é˜Ÿ
        
        high_task = asyncio.create_task(
            manager.enqueue_request(high_request, model_path, RequestPriority.HIGH)
        )
        
        # ç­‰å¾…ç»“æœ
        results = await asyncio.gather(low_task, high_task)
        
        print(f"âœ… ä½ä¼˜å…ˆçº§å“åº”: {results[0].choices[0].message.content}")
        print(f"âœ… é«˜ä¼˜å…ˆçº§å“åº”: {results[1].choices[0].message.content}")
        print("   (æ³¨æ„: é«˜ä¼˜å…ˆçº§åº”è¯¥å…ˆè¢«å¤„ç†)")
    
    async def main():
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*60)
        print("MLX-VLM æ¨¡å‹ç®¡ç†å™¨æµ‹è¯•å¥—ä»¶")
        print("="*60)
        
        try:
            # æµ‹è¯•1: æ‡’åŠ è½½
            await test_lazy_loading()
            
            # æµ‹è¯•2: ç®€å•èŠå¤©
            await test_simple_chat()
            
            # æµ‹è¯•3: æµå¼èŠå¤©
            await test_streaming_chat()
            
            # æµ‹è¯•4: ä¼˜å…ˆçº§é˜Ÿåˆ—
            await test_priority_queue()
            
            print("\n" + "="*60)
            print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
            print("="*60)
            
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())