
# 多向量检索器方案

> option 1: 文本、表格、图像都转变为多模态embeddings，图像单独保存为文件，经过相似度检索后把图像文件和原始文本提供给多模态大模型进行答案合成。
> option 2: 文本、表格、图像均使用带有视觉能力的VLLM(或多模态模型)进行描述或摘要，纯文本向量化，经过相似度检索后把表格、图像描述和原始文本给到纯大语言模型LLM进行答案合成。(原文章说用LLM而不得不将图像排除，我认为可以利用在线多模态大模型如Google Gemini，以尽量保留图像内信息)
> option 3: 文本、表格、图像均使用带有视觉能力的LLM(或多模态模型)进行描述或摘要，纯文本向量化，经过相似度检索后把表格、图像文件本身和原始文本给到多模态模型进行答案合成。

针对你给出的前端交互设计：“用户在主工作区的文件列表项上pin了一个文件，交互上是固定了一个文件在列表上方，实质上后端程序开始分析其内容并进行知识的提取，并在后续的聊天中提供参考”，我给出的核心建议：采用 Option 3 的混合架构。
Option 3 是最适合您产品定位和未来愿景的选择。 它完美地平衡了本地处理的隐私性、检索效率与在线模型的强大理解能力。

## 这是一个建议的实施架构

第一阶段：Pin 操作（本地优先，快速响应）
当用户 Pin 一个文件时，您的 Python 后端执行以下操作：

内容解析与分块: 将文档（PPT, PDF, DOCX）解析为文本块、图像和表格。

本地智能处理 (MLX/Llama.cpp/ollama/lm_studio):

文本: 使用一个轻量级的纯文本 embedding 模型进行向量化。

图像/表格: 调用本地的 Qwen-2.5-VL-7B 模型，为每个图像/表格生成丰富的文本描述。这些描述应包含尽可能多的细节。

向量化描述: 使用与文本相同的 embedding 模型，将这些生成的描述也进行向量化。

数据存储:

在向量数据库中，存储所有文本块和图像/表格描述的向量。

在元数据存储（SQLite）中，保存以下关系：

chunk_id -> original_text_content

chunk_id -> image_description -> original_image_path

chunk_id -> table_description -> original_table_data (e.g., as CSV/JSON) OR original_image_path

前端反馈: 进度条走完，告诉用户文件已“准备就绪”。这个过程完全在本地完成，保护了用户隐私。

第二阶段：对话交互（智能混合 RAG）
当用户在聊天框中提问时：

本地检索: 将用户的问题向量化（使用相同的 embedding 模型），在向量数据库中进行相似度搜索。

上下文构建与决策:

检索返回一批 chunk_id。根据 ID，从元数据存储中提取对应的原始文本、图像描述和表格描述。

智能决策点:

简单问题/隐私模式: 如果检索到的都是纯文本，或者问题比较简单，可以尝试调用本地的 gemma-3n-e4b 或 Qwen-2.5-VL-7B 来生成一个快速的、完全离线的答案。

复杂/视觉问题: 如果检索结果中包含图像/表格描述，或者用户问题本身就很复杂（需要总结、对比、分析），则触发在线流程。

在线高质量合成 (Gemini API):

根据检索到的 chunk_id，提取原始文本和原始图像文件/表格数据。

将用户问题、原始文本上下文、原始图像文件（Gemini API 支持直接传入图像数据）一起发送给 Gemini Pro (或 Flash，取决于成本和速度需求)。

将 Gemini 返回的结果呈现给用户。

## 基于强大的Python库 `Docling`  的“父文档”策略

这是目前社区中非常流行且高效的一种方法，可以看作是多向量检索器的一种标准实现。

### 分层分块 (Hierarchical Chunking)

- 步骤A (定义父块): 在 Pin 文件后，首先使用 Docling 将文档解析成一系列逻辑元素（段落、表格、图片等）。这些完整的逻辑元素，我们就定义为“父块”（Parent Chunks）。比如，一个完整的表格是一个父块，一个包含三段文字的逻辑部分也可以是一个父块。

- 步骤B (生成子块): 针对每一个“父块”，我们生成用于检索的“子块”（Child Chunks）。

如果父块是表格或图片，子块就是您用本地 VLLM (Qwen-2.5-VL-7B) 生成的摘要或描述。

如果父块是长文本段落，子块可以是这个段落的精简摘要（用 LLM 生成），或者是更小的、代表核心观点的“命题”（Propositions）。

- 步骤C (存储与关联): 将所有“子块”向量化并存入向量数据库。同时，在元数据中牢固地建立映射关系：child_chunk_id -> parent_chunk_id。

### 检索与合成

当用户提问时，在“子块”的向量空间中进行检索。

召回最相关的 N 个子块后，通过映射关系找到它们对应的 N 个“父块”。

将这些完整的、带有丰富上下文的“父块”（原始表格、原始文本段落、原始图片等）一起放入 Gemini 的上下文中，进行答案合成。

优点: 此方案清晰、可控，既保证了检索的精准度，又确保了合成时上下文的完整性，是您当前阶段的绝佳选择。

## 我们深入测试docling后发现有三种做法

我们发现不同的参数组合能生成不同的Markdown文件内容组合，摘取其中一段结果来说明：

```markdown

幸运的是，具有相同前缀的上下⽂可以利⽤ KV 缓存，这⼤⼤减少了 ⾸个 token 的⽣成 时间 (TTFT) 和推理成本 --⽆论你是使⽤⾃托管模型还是调⽤推理 API 。我们说的不是 ⼩幅度的节省：例如使⽤ Claude Sonnet 时，缓存的输⼊ token 成本为 0.30 美元 / 百万 token ，⽽未缓存的成本为 3 美元 / 百万 token --相差 10 倍。

## Design Around the KV-Cache

<!--<annotation kind="description">-->Here's a concise summary of the image, optimized for retrieval:


**Image Summary:**

The image illustrates a visualization of a Contextual Bandits algorithm, specifically focusing on the "Context @ step n" and "Context @ step n+1" stages. It shows a sequence of actions (Action 1-4) and observations (Observation 1-4), with a clear distinction between successful cache hits (green) and cache misses (red). The diagram highlights the iterative process of exploration and exploitation within a contextual bandit framework.<!--<annotation/>-->

<!-- image -->

从上下⽂⼯程的⻆度，提⾼ KV 缓存命中率涉及⼏个关键实践：

1. 保持你的提示前缀稳定。 由于 LLM 的⾃回归特性，即使是单个标记的差异也会使 该标记之后的缓存失效。⼀个常⻅的错误是在系统提示的开头包含时间戳 --尤其 是精确到秒的时间戳。虽然这让模型能告诉你当前时间，但也会降低你的缓存命中 率。

2. 使你的上下⽂只追加。 避免修改之前的操作或观察。确保你的序列化是确定性 的。许多编程语⾔和库在序列化 JSON 对象时不保证键顺序的稳定性，这可能会悄 ⽆声息地破坏缓存。

```

- A. 我可以选择不生成图片描述、无图片annotation标记、无图片占位符(Docling把表格也当图片处理)，那么md中就是纯文字部分。这种方式也许用来纯文本的chunking/embedding最合适

- B. 我可以选择生成图片描述，无图片annotation标记，无占位符，那么图片描述完全融入了正文中，就像原文就是有这些内容似的。这种方式也许是对语义更大的保留？不过要召回图片给在线多模态大模型时要通过占位符的顺序来确定是哪张图片。

- C. 我可以选择生成图片描述、有图片annotation标记，有占位符，这样可以解析annotation内图片描述进行单独的存储和embedding。看起来这是最好的选择，但在“纯文字chunking”之前要不要把annotation部分剔除掉，也就是B中我的疑问。

### 1. 三种做法的分析：如何完美处理图文关系

您的观察非常敏锐。这三种Markdown生成方式，实际上对应了我们处理多模态内容的三种不同哲学。结合我们“描述用于检索，原始数据用于合成”的目标，让我们来分析一下：

#### 做法 A (纯文字)

- **优点**: 干净、简单。非常适合对文档的纯文本部分进行分块和向量化，以建立一个纯文本的检索通道。
- **缺点**: 完全丢失了图片的存在。当用户的提问明显与图片相关时，这个检索通道是无能为力的。
- **结论**: 可以作为我们多向量检索器中的一个“通道”，但不能是全部。

#### 做法 B (描述融入正文)

- **优点**: 语义上，图片描述和上下文融合得很好。
- **缺点**: 这是一个**巨大的陷阱**。
    1. **污染上下文**: LLM生成的图片描述与原始作者的文字混杂在一起，模型在合成答案时无法区分哪些是原始信息，哪些是“AI脑补”的。
    2. **检索不精确**: 用户的查询可能会命中图片描述中的词，但这并不代表周围的原始文本与查询相关，造成了检索噪音。
    3. **难以溯源**: 就像您说的，要准确地找回对应的原始图片会变得困难和脆弱。
- **结论**: 应避免此方案。它破坏了数据的纯净性。

#### 做法 C (描述在Annotation中，带占位符)

- **优点**: 结构清晰，完美分离了原始文本、图片占位符和AI生成的描述。这为我们实现“父文档”策略提供了所有必要的“零件”。
- **缺点**: 需要在处理时多一个步骤，即解析和分离这些部分。但这完全是值得的。
- **结论**: **这是唯一正确的选择。** 它完全契合我们的核心思想。

现在，来回答您的核心疑问：

> **在“纯文字chunking”之前要不要把annotation部分剔除掉？图片与所在区域的文字关系有没有必要保留？**

答案是：**不但要剔除，还要善加利用！并且，保留图片与周围文字的关系至关重要！**

这就是“父文档”策略的精髓所在。让我们把这个流程具体化：

1. **解析与分离**: 使用`Docling`生成做法C的产物（JSON或带Annotation的Markdown）。
2. **定义“父块” (Parent Chunks)**:
    - 将连续的**纯文本段落**（已剔除所有``块）作为一个或多个“父块”。
    - 每一个**图片**（由``占位符代表）连同其周围的上下文段落，一起视为一个“父块”。这里的“周围”可以是图片紧邻的前后段落。`Docling`的JSON输出中通常会包含元素的位置信息，这可以帮助我们精确判断。
    - 每一个**表格**也是同理。
3. **创建“子块” (Child Chunks) 用于检索**:
    - 对于纯文本父块，可以将其原文直接作为子块，或者用LLM生成一个更精炼的摘要作为子块。
    - 对于图片父块，它的**图片描述 (Annotation里的内容)** 就是一个完美的子块。
    - **关键一步**：我们还可以创建一个额外的子块，即**“图片描述 + 周围原始文本的摘要”**。这个子块极大地保留了图文关系，对于回答“这张图表说明了原文中提到的哪个观点？”这类问题至关重要。
4. **存储与链接**:
    - 将所有“子块”进行向量化。
    - 在元数据中，将每个子块ID与它的父块ID（以及父块所包含的原始文本、原始图片文件路径等）牢固地关联起来。

这样，当用户提问时，我们检索的是小而精的“子块”，一旦命中，就将完整的、带有丰富上下文的“父块”调取出来，送给多模态大模型进行最终合成。

### 2. Docling 的高级分块功能：完美的起点

Docling提供了[HyBrid chunking](https://docling-project.github.io/docling/examples/hybrid_chunking/) 和 [Advanced chunking & serialization](https://docling-project.github.io/docling/examples/advanced_chunking_and_serialization/) ，其高级分块功能与我们讨论的策略惊人地契合。

> **`Docling`的 `HyBrid chunking` 和 `Advanced chunking & serialization` 是不是能在 `分层分块` 和 `检索与合成` 中直接利用呢？**

答案是：**是的，它们是实现`分层分块`的绝佳工具和起点！**

- **`HyBrid chunking`**: 这个功能允许你结合不同的分块策略（例如，按结构元素和按固定大小），这正是我们定义“父块”时所需要的。你可以先用`Docling`的内置规则将文档切分成初步的、有语义的块（比如一个标题+段落，或一个完整的表格）。
- **`Advanced chunking & serialization`**: 这个功能更强大。它允许你自定义序列化过程，比如在每个块的开头加上元数据（来源页码、元素类型等）。这对于我们后续建立“子块”到“父块”的链接至关重要。

**具体工作流建议:**

1. **使用 `Advanced chunking` 生成“父块”**:
    - 利用 `Docling` 的分块能力，将PDF解析成一系列带有元数据的、结构化的块。这些就是我们的**“父块”**。每一个块都知道自己是文本、图片还是表格，以及它在原始文档中的位置。
2. **遍历父块，生成“子块”**:
    - 写一个循环，遍历上一步生成的所有父块。
    - 如果父块是文本，直接使用或生成摘要，创建出文本“子块”。
    - 如果父块是图片，调用您代码中已有的逻辑，用VLLM生成图片描述，创建出图片描述“子块”。同时，也可以将图片周围的文本父块内容结合进来，创建出图文关系“子块”。
3. **后续流程不变**: 将所有子块向量化，建立映射关系，用于“检索与合成”。

### 3. 规划的开发步骤的分析与确认

1. 技术选型：SQLite + LanceDB (以及 SQLModel + Pydantic)

我们将采用关系型数据库 (SQLite) 和向量数据库 (LanceDB) 相结合的策略：

SQLite: 负责存储所有结构化的元数据和它们之间的关系。比如，文档信息、父块与子块的从属关系、标签、知识卡片等。它就像是整个知识库的“骨架和神经系统”。

LanceDB: 专门负责存储用于检索的向量数据。它被优化用于高效的相似度搜索，是知识库的“模式识别和反射系统”。

2. lancedb_mgr.py 管理类

- **评价**：**职责清晰**。
- **理由**：将所有与LanceDB的底层交互（创建表、插入向量、相似度搜索等）封装在一个专门的管理类中，是非常标准的“仓储模式”(Repository Pattern)。这让其他业务逻辑代码无需关心向量数据库的具体实现细节。

3. file_tagging_mgr.py 解析管理

- **评价**：**关注点分离得很好**。
- **理由**：将`Docling`的原始文件解析功能（从PDF到结构化的`DoclingDocument`对象）独立出来，是整个流程的第一步。这个模块的输入是文件路径，输出是内存中的结构化文档对象，职责非常单一。

4. multivector_mgr.py 多模态向量管理

- **评价**：**这是整个系统的核心引擎**。
- **理由**：您正确地将所有复杂的逻辑都规划到了这里。它的任务包括：
  - 接收`file_tagging_mgr`输出的文档对象。
  - 执行我们讨论的“分层分块”(Hierarchical Chunking)策略，定义“父块”和“子块”。
  - 调用本地模型或API对图片/表格生成描述（作为子块）。
  - 调用Embedding模型对所有“子块”进行向量化。
  - **编排存储**：将“父块”的元数据和内容存入SQLite，将“子块”的向量和ID存入LanceDB，并确保两者之间的关联关系被正确记录。
  - 将这个模块独立出来是完全正确的决策。

5. multivector_mgr.py 多向量检索管理

- **评价**：**RAG流程中的“召回”环节**。
- **理由**：这个模块的功能是对外的查询接口。它将：
  - 接收用户问题。
  - 对问题进行向量化。
  - 在LanceDB中执行搜索，召回最相关的“子块”ID。
  - 根据这些ID，从SQLite中查询到对应的、完整的“父块”内容（原始文本、原始图片路径等）。
  - 将这些完整的“父块”组合成最终提供给LLM的上下文。
- 这个命名也很贴切，因为它管理的正是如何从多个向量化表示中检索出最终上下文的逻辑。

6. FastAPI main.py 的API封装

- **评价**：**连接前后端的桥梁**。
- **理由**：这是将您所有后端能力暴露给Tauri前端的最终出口。规划的两个主要Endpoint非常合理：
  - **处理Endpoint (比如`/process_document`)**: 接收文件路径，然后调用`file_tagging_mgr`和`multivector_mgrr`完成整个文档的入库流程。这是一个异步的、可能耗时较长的操作。
  - **召回Endpoint (比如`/query`)**: 接收用户问题，调用`multivector_mgr`，快速返回用于合成答案的上下文。
